{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debarghaBhattacharjee/drl_for_tbg/blob/main/drl_for_tbg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Reinforcement Learning for Text-based Games"
      ],
      "metadata": {
        "id": "B03ovSeO81qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary libraries"
      ],
      "metadata": {
        "id": "7sx42bby9Em4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axff_wETVvIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccea5451-145d-4056-bd27-0214fb4a4d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [910 kB]\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,937 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,369 kB]\n",
            "Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,533 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,311 kB]\n",
            "Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,095 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,073 kB]\n",
            "Fetched 14.5 MB in 4s (3,747 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "26 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "curl is already the newest version (7.58.0-2ubuntu3.19).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.12).\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "python3-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libffi-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 26 not upgraded.\n",
            "Need to get 156 kB of archives.\n",
            "After this operation, 362 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libffi-dev amd64 3.2.1-8 [156 kB]\n",
            "Fetched 156 kB in 1s (166 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libffi-dev:amd64.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../libffi-dev_3.2.1-8_amd64.deb ...\n",
            "Unpacking libffi-dev:amd64 (3.2.1-8) ...\n",
            "Setting up libffi-dev:amd64 (3.2.1-8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update && sudo apt install build-essential libffi-dev python3-dev curl git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textworld"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lt8aMLH5n_n",
        "outputId": "ae53b2a0-7b6a-4758-d0e0-4cf050020d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textworld\n",
            "  Downloading textworld-1.5.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.10.11 in /usr/local/lib/python3.7/dist-packages (from textworld) (0.25.1)\n",
            "Collecting jericho>=3.0.3\n",
            "  Downloading jericho-3.1.0.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit in /usr/local/lib/python3.7/dist-packages (from textworld) (2.0.10)\n",
            "Collecting mementos>=1.3.1\n",
            "  Downloading mementos-1.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from textworld) (8.14.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from textworld) (1.15.1)\n",
            "Collecting hashids>=1.2.0\n",
            "  Downloading hashids-1.3.1-py2.py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.7/dist-packages (from textworld) (2.6.3)\n",
            "Requirement already satisfied: tqdm>=4.17.1 in /usr/local/lib/python3.7/dist-packages (from textworld) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from textworld) (1.21.6)\n",
            "Collecting tatsu>=4.3.0\n",
            "  Downloading TatSu-4.4.0-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->textworld) (2.21)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.11->textworld) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.11->textworld) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.11->textworld) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.10.11->textworld) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.10.11->textworld) (4.1.1)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from jericho>=3.0.3->textworld) (3.4.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (8.1.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (1.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (1.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (1.9.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (0.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (3.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.4.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (3.0.7)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.1.0->jericho>=3.0.3->textworld) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.1.0->jericho>=3.0.3->textworld) (2.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit->textworld) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit->textworld) (1.15.0)\n",
            "Building wheels for collected packages: jericho\n",
            "  Building wheel for jericho (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jericho: filename=jericho-3.1.0-py3-none-any.whl size=333859 sha256=9090969b227f91f29ced3f3b3861d8d4d7b9b6b77fbdad6289a9d06bfcda501c\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/3f/1e/b279c7bb3ec04ceeb150611eb9c2eed35c7eb150132d14d4ac\n",
            "Successfully built jericho\n",
            "Installing collected packages: tatsu, mementos, jericho, hashids, textworld\n",
            "Successfully installed hashids-1.3.1 jericho-3.1.0 mementos-1.3.1 tatsu-4.4.0 textworld-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create coin-collector game"
      ],
      "metadata": {
        "id": "YGRjrxFE9cv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !tw-make tw-coin_collector --level 5 -f -v --seed 0 --output \"/content/drive/MyDrive/text_world_games/tw_games/training/coin_collector-l5.ulx\""
      ],
      "metadata": {
        "id": "o5IqzO-D4lsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !tw-extract vocab -f -v \"/content/drive/MyDrive/text_world_games/tw_games/training/coin_collector-l5.ulx\" --output \"/content/drive/MyDrive/text_world_games/tw_games/training/coin_collector-l5-vocab.txt\""
      ],
      "metadata": {
        "id": "UJeg_RdxZ9IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play text-based games using DRL agent"
      ],
      "metadata": {
        "id": "lTvADnBd-OxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic imports"
      ],
      "metadata": {
        "id": "6EGYbVZC-V3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import glob\n",
        "import time, datetime\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "Re_OFEoV-VjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration/Setup file"
      ],
      "metadata": {
        "id": "ra0akxUQ_fJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self, warm_start=False):\n",
        "        # Experimental setup\n",
        "        self.game_name = \"coin_collector-easy-l5-seed_1\"  # Default: \"coin_collector-l5\"\n",
        "        self.root_dir = \"/content/drive/MyDrive/text_world_games\"\n",
        "        self.game_file = f\"{self.root_dir}/tw_games/training/{self.game_name}.ulx\"\n",
        "        # self.vocab_file = f\"{self.root_dir}/tw_games/training/{self.game_name}-vocab.txt\"  # Default\n",
        "        self.vocab_file = \"/content/drive/MyDrive/text_world_games/tw_games/training/coin_collector-l5-vocab.txt\"  # Explicitly specified\n",
        "        self.nb_epochs = 200\n",
        "        self.nb_episodes = 50\n",
        "        self.max_episode_steps = 50\n",
        "        self.result_dir = f\"{self.root_dir}/{self.game_name}/result_dir\"\n",
        "        self.log_dir = f\"{self.root_dir}/{self.game_name}/log_dir\"\n",
        "        self.save_log_freq = 5\n",
        "        self.ckpt_dir = f\"{self.root_dir}/{self.game_name}/model_dir\"\n",
        "        self.save_ckpt_freq = 10\n",
        "        self.plot_dir = f\"{self.root_dir}/{self.game_name}/plot_dir\"\n",
        "\n",
        "        self.device = torch.device(\n",
        "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        )\n",
        "        print(f\"Device available: {self.device}\")\n",
        "\n",
        "        # Actor network parameters\n",
        "        self.max_seq_len = 128\n",
        "        self.embedding_size = 64\n",
        "        self.embeddings = None # Specify full path to embeddings file (if it exists)\n",
        "        self.freeze_embeddings = False\n",
        "        self.hidden_size = 64\n",
        "        self.nb_layers = 1\n",
        "        self.a_lr = 1e-3 \n",
        "        if warm_start:\n",
        "            # Reduce by a factor of 10 (i.e., 1/10th) when warm starting \n",
        "            self.a_lr = self.a_lr/10\n",
        "        self.max_grad_norm = 5.0\n",
        "        self.t_update_freq = 5\n",
        "\n",
        "        # Replay buffer parameters\n",
        "        self.alpha_beta_replay = True\n",
        "        self.max_buffer_len = 1_000_000 # alpha buffer max len + beta buffer max len \n",
        "        self.alpha_storage_fraction = 0.25\n",
        "        self.alpha_sampling_fraction = 0.50\n",
        "        self.alpha_threshold = 0.00\n",
        "        self.replay_batch_size = 128\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        ## Action selection strategy\n",
        "        self.epsilon_init = 1.00\n",
        "        self.epsilon_end = 0.05\n",
        "        self.e_greedy_stop = 0.40 * self.nb_epochs\n",
        "        self.epsilon_decay_rate = (\n",
        "            self.epsilon_init - self.epsilon_end\n",
        "        ) / (self.e_greedy_stop - 1)"
      ],
      "metadata": {
        "id": "BuL5nKVBSmwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create text world game environment object"
      ],
      "metadata": {
        "id": "tCgW4U2w9rIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import textworld\n",
        "import textworld.gym"
      ],
      "metadata": {
        "id": "oO0KiLSB5-ML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44fa7516-5ad4-4cd9-b027-ef8e9495a065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tatsu/grammars.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import defaultdict, Mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextWorldGame:\n",
        "    def __init__(self, config):\n",
        "        self.name = config.game_name\n",
        "        self.game_files = [config.game_file]\n",
        "        if os.path.isfile(config.game_file):\n",
        "            self.game_files = glob.glob(config.game_file)\n",
        "        self.max_episode_steps = config.max_episode_steps\n",
        "        self.env_id = textworld.gym.register_games(\n",
        "            gamefiles=self.game_files,\n",
        "            request_infos=self.request_infos(),\n",
        "            max_episode_steps=self.max_episode_steps\n",
        "        )\n",
        "        self.env = gym.make(self.env_id, new_step_api=True, disable_env_checker=True)\n",
        "\n",
        "        if os.path.isfile(config.vocab_file):\n",
        "            self.vocab_files = glob.glob(config.vocab_file)\n",
        "            assert(len(self.vocab_files) ==  1)\n",
        "        self.vocab = self.get_vocab(vocab_file=self.vocab_files[0])\n",
        "\n",
        "\n",
        "    def get_game_info(self):\n",
        "        game_info = {\n",
        "            \"env\": self.env,\n",
        "            \"name\": self.name,\n",
        "            \"max_episode_steps\": self.max_episode_steps,\n",
        "            \"vocab\": self.vocab\n",
        "        }\n",
        "        return game_info\n",
        "\n",
        "\n",
        "    def get_vocab(self, vocab_file):\n",
        "        vocab = list()\n",
        "        with open(vocab_file, \"r\") as f:\n",
        "            tokens = f.readlines()\n",
        "            vocab = [re.sub(r\"\\s\", \"\", token.lower()) for token in tokens]\n",
        "            vocab.remove(\"\")\n",
        "        return vocab\n",
        "\n",
        "\n",
        "    def get_game_controls(self):\n",
        "        _, info = self.env.reset() \n",
        "        game_controls = {\n",
        "            \"verbs\": info.get(\"verbs\", None),\n",
        "            \"entities\": info.get(\"entities\", None)\n",
        "        }\n",
        "        return game_controls\n",
        "        \n",
        "\n",
        "    def request_infos(self):\n",
        "        request_infos = textworld.EnvInfos()\n",
        "        request_infos.description = True\n",
        "        request_infos.last_command = True\n",
        "        request_infos.won = True\n",
        "        request_infos.verbs = True\n",
        "        request_infos.entities = True\n",
        "        return request_infos"
      ],
      "metadata": {
        "id": "H6VsBrIP-CKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random agent"
      ],
      "metadata": {
        "id": "EzLlPTAZFnEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAgent:\n",
        "    def __init__(self, config, vocab, verbs, entities):\n",
        "        self.agent = \"random_agent\"\n",
        "        self.name = config.game_name\n",
        "        self.nb_epochs = config.nb_epochs\n",
        "        self.nb_episodes = config.nb_episodes\n",
        "        self.max_episode_steps = config.max_episode_steps\n",
        "\n",
        "        # Results directory\n",
        "        self.result_dir = f\"{config.result_dir}/{self.agent}\"\n",
        "        if not os.path.isdir(self.result_dir):\n",
        "            os.makedirs(self.result_dir)\n",
        "\n",
        "        # Log directory\n",
        "        self.log_dir = f\"{config.log_dir}/{self.agent}\"\n",
        "        if not os.path.isdir(self.log_dir):\n",
        "            os.makedirs(self.log_dir)\n",
        "        self.summary_writer = SummaryWriter(log_dir=self.log_dir)\n",
        "        self.save_log_freq = config.save_log_freq\n",
        "\n",
        "        # Checkpoint directory\n",
        "        self.ckpt_dir = f\"{config.model_dir}/{self.agent}\"\n",
        "        if not os.path.isdir(self.ckpt_dir):\n",
        "            os.makedirs(self.ckpt_dir)\n",
        "        self.save_ckpt_freq = config.save_model_freq\n",
        "\n",
        "        # Vocab: for dealing with the state space\n",
        "        self.id2tok = {0:\"<pad>\", 1:\"<unk>\", 2:\"<sos>\", 3:\"<eos>\"}\n",
        "        for (id, tok) in enumerate(vocab, start=4):\n",
        "            self.id2tok[id] = tok\n",
        "        self.tok2id = {tok:id for (id, tok) in enumerate(self.id2tok)}\n",
        "\n",
        "        # Verbs and entities: for dealing with action space\n",
        "        self.id2verb = {id:verb for (id, verb) in enumerate(verbs)}\n",
        "        self.verb2id = {verb:id for (id, verb) in enumerate(verbs)}\n",
        "        self.id2entity = {id:entity for (id, entity) in enumerate(entities)}\n",
        "        self.entity2id = {entity:id for (id, entity) in enumerate(entities)}\n",
        "\n",
        "        # # Annealing epsilon-greedy policy configuration\n",
        "        # self.epsilon_init = config.epsilon_init\n",
        "        # self.epsilon_end = config.epsilon_end\n",
        "        # self.epsilon_decay_rate = config.epsilon_decay_rate\n",
        "        # self.epsilon = config.epsilon\n",
        "\n",
        "    \n",
        "    def save_data(self, data, file_name, directory):\n",
        "        # Create directory if it does not exist.\n",
        "        if not os.path.isdir(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        # Save data as pickled object.\n",
        "        pickle_out = open(\n",
        "            f\"{directory}/{file_name}\", \"wb\"\n",
        "        )\n",
        "        pickle.dump(data, pickle_out)\n",
        "        pickle_out.close()\n",
        "        return\n",
        "\n",
        "\n",
        "    def get_command(self):\n",
        "        \"\"\"\n",
        "        This method returns the command that should be issued\n",
        "        by the random agent.\n",
        "        \"\"\"\n",
        "        # Select verb\n",
        "        v_id = np.random.randint(low=0, high=len(self.verb2id), size=1)\n",
        "        verb = self.id2verb.get(v_id.item(), None)\n",
        "        # Select entity\n",
        "        e_id = np.random.randint(low=0, high=len(self.entity2id), size=1)\n",
        "        entity = self.id2entity.get(e_id.item(), None)\n",
        "        command = f\"{verb} {entity}\"\n",
        "        return command\n",
        "\n",
        "    def test(self, test_env):\n",
        "        test_stats = {\n",
        "            \"avg_reward\": [],\n",
        "            \"avg_steps\" : [],\n",
        "            \"avg_wins\"  : []\n",
        "        }\n",
        "\n",
        "        # Initialize empty list for storing episode info at \n",
        "        # the start of every epoch.\n",
        "        e_rewards = []\n",
        "        e_steps   = []\n",
        "        e_wins    = []\n",
        "\n",
        "        for episode in range(self.nb_episodes):\n",
        "            # Reset at the beginning of every episode.\n",
        "            obs, info = test_env.reset()\n",
        "            prev_score = 0\n",
        "            cum_sum_reward = 0\n",
        "            nb_steps = 0\n",
        "            done = False\n",
        "            won = False\n",
        "\n",
        "            while not done:\n",
        "                command = self.get_command()\n",
        "                obs, score, done, _, info = test_env.step(command)\n",
        "                reward = score - prev_score\n",
        "                cum_sum_reward += reward\n",
        "                nb_steps += 1\n",
        "                won = info[\"won\"]\n",
        "                if nb_steps >= self.max_episode_steps:\n",
        "                    break\n",
        "                prev_score = score\n",
        "            \n",
        "            e_rewards.append(cum_sum_reward) \n",
        "            e_steps.append(nb_steps)\n",
        "            e_wins.append(won)\n",
        "\n",
        "        test_stats[\"avg_reward\"] = np.nanmean(e_rewards) \n",
        "        test_stats[\"avg_steps\"] = np.nanmean(e_steps)\n",
        "        test_stats[\"avg_wins\"] = np.nanmean(e_wins)\n",
        "        return test_stats\n",
        "\n",
        "\n",
        "    def play(self, test_env):\n",
        "        \"\"\"\n",
        "        Play the game using the agent.\n",
        "        \"\"\"\n",
        "        ep_stats = {\n",
        "            \"avg_reward_test\": [],\n",
        "            \"avg_steps_test\" : [],\n",
        "            \"avg_wins_test\"  : []\n",
        "        }\n",
        "\n",
        "        epoch_offset = 0\n",
        "        for epoch in tqdm(range(epoch_offset+1, self.nb_epochs+1), unit=\" epoch\"):\n",
        "            start = time.time()\n",
        "            ep_test_stats = self.test(test_env)\n",
        "            if ((epoch == 0) or ((epoch % self.save_log_freq) == 0)):\n",
        "                msg = f\"Epoch: {epoch}\\n\" + \\\n",
        "                    f\"Test avg. reward: {np.around(ep_test_stats['avg_reward'], 2)} \" + \\\n",
        "                    f\"Test avg. steps: {np.around(ep_test_stats['avg_steps'], 2)} \" + \\\n",
        "                    f\"Test avg. wins: {np.around(ep_test_stats['avg_wins'], 2)}\"\n",
        "                print(msg)\n",
        "                print(\"=\" * 75)\n",
        "            self.summary_writer.add_scalar(\"avg. reward\", ep_test_stats[\"avg_reward\"])\n",
        "            self.summary_writer.add_scalar(\"avg. steps\", ep_test_stats[\"avg_steps\"])\n",
        "            self.summary_writer.add_scalar(\"avg. wins\", ep_test_stats[\"avg_wins\"])\n",
        "            \n",
        "            ep_stats[\"avg_reward_test\"].append(ep_test_stats[\"avg_reward\"])\n",
        "            ep_stats[\"avg_steps_test\"].append(ep_test_stats[\"avg_steps\"])\n",
        "            ep_stats[\"avg_wins_test\"].append(ep_test_stats[\"avg_wins\"])\n",
        "            self.save_data(\n",
        "                data=ep_stats, file_name=\"ep_stats\", \n",
        "                directory=self.result_dir\n",
        "            )\n",
        "        return ep_stats"
      ],
      "metadata": {
        "id": "MjwaZLUt51Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test random agent"
      ],
      "metadata": {
        "id": "WZBVf5gLJAlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set random generator seed for reproducibility."
      ],
      "metadata": {
        "id": "S-KahwO7_mvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5PGYojQ_BKu",
        "outputId": "1bd163bf-59d2-4181-d4e0-f795a1adea1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f092f570f10>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get test environment and random agent"
      ],
      "metadata": {
        "id": "FePPz1lPJveA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "play_random_agent = False  # Set to True if you want to play using random agent.\n",
        "if play_random_agent:\n",
        "    config = Config()\n",
        "    text_world_game = TextWorldGame(config=config)\n",
        "    random_agent = RandomAgent(\n",
        "        config=config,\n",
        "        vocab=text_world_game.get_game_info().get(\"vocab\", None), \n",
        "        verbs=[\"go\", \"take\"], # Pruned the verb space to simplify the problem\n",
        "        # verbs=text_world_game.get_game_controls().get(\"verbs\", None), \n",
        "        entities=text_world_game.get_game_controls().get(\"entities\", None), \n",
        "    )\n",
        "    test_env = text_world_game.get_game_info().get(\"env\", None)"
      ],
      "metadata": {
        "id": "HiTsC1YF6jIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play random agent in the test environment"
      ],
      "metadata": {
        "id": "c5Ag9NTWOwoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if play_random_agent:\n",
        "    random_agent_stats = random_agent.play(test_env=test_env)\n",
        "    random_agent_stats_df = pd.DataFrame(random_agent_stats)\n",
        "    random_agent_stats_df[\"epoch\"] = np.arange(start=1, stop=random_agent_stats_df.shape[0]+1)\n",
        "    random_agent_stats_df"
      ],
      "metadata": {
        "id": "H2IOaRHJ6i4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if play_random_agent:\n",
        "    plot_dir = f\"{config.plot_dir}/random_agent\"\n",
        "    if not os.path.isdir(plot_dir):\n",
        "        os.makedirs(plot_dir)\n",
        "\n",
        "    sns.lineplot(x=\"epoch\", y=\"avg_reward_test\", data=random_agent_stats_df);\n",
        "    plt.savefig(f\"{plot_dir}/avg_reward_test.jpeg\");\n",
        "\n",
        "    sns.lineplot(x=\"epoch\", y=\"avg_steps_test\", data=random_agent_stats_df);\n",
        "    plt.savefig(f\"{plot_dir}/avg_steps_test.jpeg\");\n",
        "\n",
        "    sns.lineplot(x=\"epoch\", y=\"avg_wins_test\", data=random_agent_stats_df);\n",
        "    plt.savefig(f\"{plot_dir}/avg_wins_test.jpeg\");"
      ],
      "metadata": {
        "id": "6sjXc1WbsUZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RL Agent\n",
        "The RL agent uses a neural network function approximator as the policy network. Specifically, we use the LSTM-DQN model. Unlike the random agent, the RL agent leverages intelligent decision making for playing text-based games."
      ],
      "metadata": {
        "id": "wlpguOFBd1PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Replay memory"
      ],
      "metadata": {
        "id": "40neARvOwPUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple, deque"
      ],
      "metadata": {
        "id": "cObBe2hUwPBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Experience = namedtuple(\"Experience\", (\n",
        "    \"cur_state\", \"verb\", \"entity\", \"reward\",\n",
        "    \"next_state\", \"done\"\n",
        "))"
      ],
      "metadata": {
        "id": "9mvsLtU1wqwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaBetaReplayBuffer(object):\n",
        "    def __init__(self, max_len=1_000_000, alpha_storage_fraction=0.25, \n",
        "                 alpha_sampling_fraction=0.50):\n",
        "        super(AlphaBetaReplayBuffer, self).__init__()\n",
        "        self.max_len = max_len\n",
        "        self.alpha_storage_fraction = alpha_storage_fraction\n",
        "        self.alpha_sampling_fraction = alpha_sampling_fraction\n",
        "        # Alpha buffer modules\n",
        "        # -------------------------------------------------------------\n",
        "        self.alpha_max_len = int(self.max_len * self.alpha_storage_fraction)\n",
        "        self.alpha_buffer = deque(maxlen=self.alpha_max_len)\n",
        "        # Beta buffer modules\n",
        "        # --------------------------------------------------------------\n",
        "        self.beta_max_len = self.max_len - self.alpha_max_len\n",
        "        self.beta_buffer = deque(maxlen=self.beta_max_len)\n",
        "\n",
        "    def push(self, is_alpha=False, *args):\n",
        "        \"\"\"Adds an experience to replay buffer.\"\"\"\n",
        "        if is_alpha:\n",
        "            self.alpha_buffer.append(Experience._make(*args))\n",
        "        else:\n",
        "            self.beta_buffer.append(Experience._make(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        alpha_batch_size = int(self.alpha_sampling_fraction * batch_size)\n",
        "        alpha_sample_size = min(len(self.alpha_buffer), alpha_batch_size)\n",
        "        alpha_samples = random.sample(self.alpha_buffer, alpha_sample_size)\n",
        "        beta_batch_size = batch_size - alpha_batch_size\n",
        "        beta_sample_size = min(len(self.beta_buffer), beta_batch_size)\n",
        "        beta_samples = random.sample(self.beta_buffer, beta_sample_size)\n",
        "        samples = alpha_samples + beta_samples\n",
        "        random.shuffle(samples)\n",
        "        return samples"
      ],
      "metadata": {
        "id": "KKWrk5MdxCMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Actor loss function\n",
        "This is the loss function for the DQN algorithm based on the least mean squared error. "
      ],
      "metadata": {
        "id": "TtkNwGAF7xN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorLoss, self).__init__()\n",
        "        \n",
        "    def forward(self, vq_error, eq_error):\n",
        "        \"\"\"\n",
        "        INPUT\n",
        "        ------------------------------\n",
        "        vq_error shape = (replay_batch_size, 1)\n",
        "        eq_error shape = (replay_batch_size, 1)\n",
        "        \"\"\"\n",
        "        vq_loss = torch.square(vq_error)\n",
        "        vq_loss = (1/2) * vq_loss.mean()\n",
        "        eq_loss = torch.square(eq_error)\n",
        "        eq_loss = (1/2) * eq_loss.mean()\n",
        "        actor_loss = vq_loss + eq_loss\n",
        "        return actor_loss"
      ],
      "metadata": {
        "id": "VJ3_8aqO7w2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Actor"
      ],
      "metadata": {
        "id": "zcjPMIs7vN9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StateEncoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, embeddings, \n",
        "                 freeze_embeddings, hidden_size, nb_layers):\n",
        "        super(StateEncoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # Embedding module\n",
        "        if embeddings is not None:\n",
        "            self.embedding = torch.nn.Embedding(\n",
        "                num_embeddings=vocab_size, embedding_dim=embedding_size\n",
        "            ).from_pretrained(\n",
        "                embeddings=embeddings, freeze=freeze_embeddings\n",
        "            )\n",
        "        else:\n",
        "            self.embedding = torch.nn.Embedding(\n",
        "                num_embeddings=vocab_size, embedding_dim=embedding_size\n",
        "            )\n",
        "\n",
        "        # Input sequence encoder module\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=embedding_size, hidden_size=hidden_size,\n",
        "            num_layers=nb_layers, batch_first=True,\n",
        "            dropout=0.0, bidirectional=True\n",
        "        )\n",
        "        self.encoder = torch.nn.Linear(\n",
        "            in_features=2*hidden_size, out_features=hidden_size\n",
        "        )\n",
        "\n",
        "    def forward(self, in_seq):\n",
        "        batch_size, mts = in_seq.shape\n",
        "        in_seq_mts = in_seq.shape[1]\n",
        "        in_seq_embed = self.embedding(in_seq)\n",
        "        in_seq_mask = torch.sign(in_seq)\n",
        "        in_seq_len = torch.sum(in_seq_mask, dim=1)\n",
        "        lstm_in = pack_padded_sequence(\n",
        "            input=in_seq_embed,\n",
        "            lengths=in_seq_len.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "        enc_seq, _ = self.lstm(lstm_in)\n",
        "        enc_seq, _ = pad_packed_sequence(\n",
        "            sequence=enc_seq,\n",
        "            batch_first=True,\n",
        "            total_length=in_seq_mts\n",
        "        ) # [b, t, d]\n",
        "        enc_seq = torch.mean(enc_seq, dim=1)\n",
        "        enc_seq = self.encoder(enc_seq)\n",
        "        return enc_seq"
      ],
      "metadata": {
        "id": "IPVd0ZhqjElF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionValueEstimator(torch.nn.Module):\n",
        "    def __init__(self, verb_space_size, entity_space_size, hidden_size):\n",
        "        super(ActionValueEstimator, self).__init__()\n",
        "        # Action-value estimator for verbs\n",
        "        self.vq_estimator_init = torch.nn.Linear(\n",
        "            in_features=hidden_size, out_features=hidden_size//2\n",
        "        )\n",
        "        self.vq_estimator_final = torch.nn.Linear(\n",
        "            in_features=hidden_size//2, out_features=verb_space_size\n",
        "        )\n",
        "        # Action-value estimator for entities\n",
        "        self.eq_estimator_init = torch.nn.Linear(\n",
        "            in_features=hidden_size, out_features=hidden_size//2\n",
        "        )\n",
        "        self.eq_estimator_final = torch.nn.Linear(\n",
        "            in_features=hidden_size//2, out_features=entity_space_size\n",
        "        )\n",
        "\n",
        "    def forward(self, in_state):\n",
        "        # Obtain the action-values for output verbs.\n",
        "        vq_vals = self.vq_estimator_init(in_state)\n",
        "        vq_vals = self.vq_estimator_final(vq_vals)\n",
        "\n",
        "        # Obtain the action-values for output entities.\n",
        "        eq_vals = self.eq_estimator_init(in_state)\n",
        "        eq_vals = self.eq_estimator_final(eq_vals)\n",
        "\n",
        "        return vq_vals, eq_vals"
      ],
      "metadata": {
        "id": "XdBEWxwkqtmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, embeddings, \n",
        "                 freeze_embeddings, hidden_size, nb_layers,\n",
        "                 verb_space_size, entity_space_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.state_encoder = StateEncoder(\n",
        "            vocab_size=vocab_size, \n",
        "            embedding_size=embedding_size, \n",
        "            embeddings=embeddings, \n",
        "            freeze_embeddings=freeze_embeddings, \n",
        "            hidden_size=hidden_size, \n",
        "            nb_layers=nb_layers\n",
        "        )\n",
        "        self.action_value_estimator = ActionValueEstimator(\n",
        "            verb_space_size=verb_space_size, \n",
        "            entity_space_size=entity_space_size, \n",
        "            hidden_size=hidden_size\n",
        "        )\n",
        "\n",
        "    def forward(self, in_seq):\n",
        "        enc_state = self.state_encoder(in_seq=in_seq)\n",
        "        vq_vals, eq_vals = self.action_value_estimator(in_state=enc_state)\n",
        "        return vq_vals, eq_vals"
      ],
      "metadata": {
        "id": "MFNW4xw4tw-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RL agent"
      ],
      "metadata": {
        "id": "iw-0KDd58eQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RLAgent:\n",
        "    def __init__(self, config, vocab, verbs, entities):\n",
        "        self.agent = \"rl_agent-complex\"\n",
        "        self.name = config.game_name\n",
        "        self.device = config.device\n",
        "        self.nb_epochs = config.nb_epochs\n",
        "        self.nb_episodes = config.nb_episodes\n",
        "        self.max_episode_steps = config.max_episode_steps\n",
        "\n",
        "        # Results directory\n",
        "        self.result_dir = f\"{config.result_dir}/{self.agent}\"\n",
        "        if not os.path.isdir(self.result_dir):\n",
        "            os.makedirs(self.result_dir)\n",
        "\n",
        "        # Log directory\n",
        "        self.log_dir = f\"{config.log_dir}/{self.agent}\"\n",
        "        if not os.path.isdir(self.log_dir):\n",
        "            os.makedirs(self.log_dir)\n",
        "        self.summary_writer = SummaryWriter(log_dir=self.log_dir)\n",
        "        self.save_log_freq = config.save_log_freq\n",
        "\n",
        "        # Checkpoint directory\n",
        "        self.ckpt_dir = f\"{config.ckpt_dir}/{self.agent}\"\n",
        "        if not os.path.isdir(self.ckpt_dir):\n",
        "            os.makedirs(self.ckpt_dir)\n",
        "        self.save_ckpt_freq = config.save_ckpt_freq\n",
        "\n",
        "        # Vocab: for dealing with the state space\n",
        "        self.id2token = {0:\"<pad>\", 1:\"<unk>\", 2:\"<sos>\", 3:\"<eos>\"}\n",
        "        for (id, token) in enumerate(vocab, start=4):\n",
        "            self.id2token[id] = token\n",
        "        self.token2id = {token:id for (id, token) in self.id2token.items()}\n",
        "\n",
        "        # Verbs and entities: for dealing with action space\n",
        "        self.id2verb = {id:verb for (id, verb) in enumerate(verbs)}\n",
        "        self.verb2id = {verb:id for (id, verb) in enumerate(verbs)}\n",
        "        self.id2entity = {id:entity for (id, entity) in enumerate(entities)}\n",
        "        self.entity2id = {entity:id for (id, entity) in enumerate(entities)}\n",
        "\n",
        "        # Actor network\n",
        "        self.max_seq_len = config.max_seq_len\n",
        "        self.actor_m = Actor(\n",
        "            vocab_size=len(self.token2id), \n",
        "            embedding_size=config.embedding_size, \n",
        "            embeddings=config.embeddings, \n",
        "            freeze_embeddings=config.freeze_embeddings, \n",
        "            hidden_size=config.hidden_size, \n",
        "            nb_layers=config.nb_layers,\n",
        "            verb_space_size=len(self.verb2id), \n",
        "            entity_space_size=len(self.entity2id)\n",
        "        ).to(config.device)\n",
        "        self.max_grad_norm = config.max_grad_norm\n",
        "        self.gamma = config.gamma\n",
        "        self.a_optimizer = torch.optim.Adam(\n",
        "            params=self.actor_m.parameters(), \n",
        "            lr=config.a_lr\n",
        "        )\n",
        "        self.a_criterion = ActorLoss()\n",
        "        self.actor_t = Actor(\n",
        "            vocab_size=len(self.token2id), \n",
        "            embedding_size=config.embedding_size, \n",
        "            embeddings=config.embeddings, \n",
        "            freeze_embeddings=config.freeze_embeddings, \n",
        "            hidden_size=config.hidden_size, \n",
        "            nb_layers=config.nb_layers,\n",
        "            verb_space_size=len(self.verb2id), \n",
        "            entity_space_size=len(self.entity2id)\n",
        "        ).to(config.device)\n",
        "        self.actor_t.load_state_dict(self.actor_m.state_dict())\n",
        "        self.t_update_freq = config.t_update_freq\n",
        "\n",
        "        # Replay buffer parameters\n",
        "        self.alpha_threshold = config.alpha_threshold\n",
        "        self.replay_buffer = AlphaBetaReplayBuffer(\n",
        "            max_len=config.max_buffer_len,\n",
        "            alpha_storage_fraction=config.alpha_storage_fraction,\n",
        "            alpha_sampling_fraction=config.alpha_sampling_fraction\n",
        "        )\n",
        "        self.replay_batch_size = config.replay_batch_size\n",
        "\n",
        "        # Annealing epsilon-greedy policy configuration\n",
        "        self.epsilon_init = config.epsilon_init\n",
        "        self.epsilon_end = config.epsilon_end\n",
        "        self.epsilon_decay_rate = config.epsilon_decay_rate\n",
        "        self.epsilon = self.epsilon_init\n",
        "\n",
        "        # Load checkpoint (if required).\n",
        "        self.epoch_offset = 0\n",
        "        ckpt_available = False  # Set to True if you want to load checkpoint.\n",
        "        if ckpt_available == True:\n",
        "            self.load_ckpt()\n",
        "\n",
        "\n",
        "    def load_ckpt(self):\n",
        "        \"\"\"\n",
        "        This methop loads a previously saved checkpoint (if required \n",
        "        and avialble) and resumes training from that point. \n",
        "        \"\"\"\n",
        "        ckpt = torch.load(f\"{self.ckpt_dir}/ckpt\")\n",
        "        self.epoch_offset = ckpt[\"epoch\"]\n",
        "        self.actor_m.load_state_dict(ckpt[\"actor_m_state_dict\"])\n",
        "        self.actor_t.load_state_dict(ckpt[\"actor_m_state_dict\"])\n",
        "        self.a_optimizer.load_state_dict(ckpt[\"a_optimizer_state_dict\"])\n",
        "        self.epsilon = ckpt[\"epsilon\"]\n",
        "        print(f\"Loaded checkpoint from: {self.ckpt_dir}/ckpt\")\n",
        "        return\n",
        "\n",
        "\n",
        "    def save_ckpt(self, epoch):\n",
        "        ckpt = {\n",
        "            \"epoch\": epoch,\n",
        "            \"actor_m_state_dict\": self.actor_m.state_dict(),\n",
        "            \"a_optimizer_state_dict\": self.a_optimizer.state_dict(),\n",
        "            \"epsilon\": self.epsilon\n",
        "        }\n",
        "        torch.save(ckpt, f\"{self.ckpt_dir}/ckpt\")\n",
        "        return\n",
        "\n",
        "\n",
        "    def copy_actor_weights(self, src_path=None):\n",
        "        \"\"\"\n",
        "        This method copy weights from a another rl agent's \n",
        "        actor network to current agent's actor network.\n",
        "        \"\"\"\n",
        "        if src_path is None:\n",
        "            print(\"No source provided. Couldn't copy weights.\")\n",
        "            return\n",
        "        src_actor = torch.load(src_path)\n",
        "        self.actor_m.load_state_dict(src_actor[\"actor_m_state_dict\"])\n",
        "        self.actor_t.load_state_dict(src_actor[\"actor_m_state_dict\"])\n",
        "        print(f\"Sucessfully copied weights from: {src_path}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    def save_data(self, data, file_name, directory):\n",
        "        # Create directory if it does not exist.\n",
        "        if not os.path.isdir(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        # Save data as pickled object.\n",
        "        pickle_out = open(\n",
        "            f\"{directory}/{file_name}\", \"wb\"\n",
        "        )\n",
        "        pickle.dump(data, pickle_out)\n",
        "        pickle_out.close()\n",
        "        return\n",
        "\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = re.sub(r\"[^a-zA-Z0-9\\-'<> ]\", r\" \", text.lower())\n",
        "        return text.strip()\n",
        "        \n",
        "    \n",
        "    def get_token_id(self, token):\n",
        "        return self.token2id.get(token, self.token2id[\"<unk>\"])\n",
        "    \n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        token_ids = list(map(self.get_token_id, text.split()))\n",
        "        return token_ids\n",
        "\n",
        "\n",
        "    def make_tensor(self, text):\n",
        "        token_ids_list = self.tokenize_text(text)\n",
        "        tensor = torch.tensor([token_ids_list]).to(self.device)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "    def get_padded_tensors(self, tensors_list):\n",
        "        # print(f\"tensors_list: \\n{tensors_list}\")\n",
        "        # print(\"-\" * 75)\n",
        "        padded_tensors = pad_sequence(\n",
        "            sequences=tensors_list,\n",
        "            batch_first=True,\n",
        "            padding_value=self.token2id[\"<pad>\"]\n",
        "        ).to(self.device)\n",
        "        return padded_tensors\n",
        "\n",
        "\n",
        "    def get_train_command(self, in_seq):\n",
        "        \"\"\"\n",
        "        This method returns the command that should be issued\n",
        "        by the RL agent in the training phase.\n",
        "        \"\"\"\n",
        "        self.actor_m.eval()\n",
        "        batch_size = in_seq.shape[0]\n",
        "        command = \"\" \n",
        "        action_ids = {\"v_id\": 0, \"e_id\": 0}\n",
        "        with torch.no_grad():\n",
        "            vq_vals, eq_vals = self.actor_m(in_seq=in_seq)\n",
        "            prob = random.random()\n",
        "            if prob > self.epsilon:\n",
        "                _, v_id = vq_vals.topk(k=1, dim=1)\n",
        "                _, e_id = eq_vals.topk(k=1, dim=1)\n",
        "            else:\n",
        "                v_id = torch.randint(low=0, high=vq_vals.shape[1], size=(batch_size, 1))\n",
        "                e_id = torch.randint(low=0, high=eq_vals.shape[1], size=(batch_size, 1))\n",
        "            # Select verb\n",
        "            verb = self.id2verb.get(v_id.item(), None)\n",
        "            # Select entity\n",
        "            entity = self.id2entity.get(e_id.item(), None)\n",
        "            command = f\"{verb} {entity}\"\n",
        "            action_ids[\"v_id\"] = v_id\n",
        "            action_ids[\"e_id\"] = e_id\n",
        "        return command, action_ids\n",
        "\n",
        "    \n",
        "    def save_experience(self, experience, is_alpha):\n",
        "        \"\"\"\n",
        "        This methods saves the experience in the in either the\n",
        "        stream or beta stream of the replay buffer depending on\n",
        "        is_alpha flag.\n",
        "        \"\"\"\n",
        "        self.replay_buffer.push(is_alpha, experience)\n",
        "        return\n",
        "\n",
        "\n",
        "    def sample_experiences(self):\n",
        "        # Sample replay_batch_size no. of stored experiences from Replay Memory.\n",
        "        replay_batch = self.replay_buffer.sample(batch_size=self.replay_batch_size)\n",
        "        if (len(replay_batch) < self.replay_batch_size):\n",
        "            return None\n",
        "        \n",
        "        # Extract necessary information.\n",
        "        cur_state  = [data.cur_state for data in replay_batch] # Current state\n",
        "        verb       = [data.verb for data in replay_batch]  # Verb\n",
        "        entity     = [data.entity for data in replay_batch]  # Entity\n",
        "        reward     = [data.reward for data in replay_batch]  # Reward\n",
        "        next_state = [data.next_state for data in replay_batch]  # Next state\n",
        "        done       = [data.done for data in replay_batch]  # Done\n",
        "\n",
        "        # Get tensor representaions.\n",
        "        cur_state_tensor = self.get_padded_tensors(cur_state)\n",
        "        verb_tensor = torch.tensor(verb).view(self.replay_batch_size, 1).to(self.device)\n",
        "        entity_tensor = torch.tensor(entity).view(self.replay_batch_size, 1).to(self.device)\n",
        "        reward_tensor = torch.tensor(reward).view(self.replay_batch_size, 1).to(self.device)\n",
        "        next_state_tensor = self.get_padded_tensors(next_state)\n",
        "        done_tensor = torch.tensor(done).view(self.replay_batch_size, 1).to(self.device)\n",
        "        return cur_state_tensor, verb_tensor, entity_tensor, \\\n",
        "            reward_tensor, next_state_tensor, done_tensor\n",
        "\n",
        "    \n",
        "    def compute_actor_loss(self, replay_batch):\n",
        "        cur_state_tensor, verb_tensor, entity_tensor, \\\n",
        "        reward_tensor, next_state_tensor, done_tensor = replay_batch\n",
        "        pred_vq_vals, pred_eq_vals = self.actor_m(in_seq=cur_state_tensor)\n",
        "        pred_vq_val = pred_vq_vals.gather(index=verb_tensor, dim=1)\n",
        "        pred_eq_val = pred_eq_vals.gather(index=entity_tensor, dim=1)\n",
        "\n",
        "        next_vq_vals, next_eq_vals = self.actor_m(in_seq=next_state_tensor)\n",
        "        next_v_id = next_vq_vals.argmax(dim=1).unsqueeze(dim=1)\n",
        "        next_e_id = next_eq_vals.argmax(dim=1).unsqueeze(dim=1)\n",
        "\n",
        "        next_vq_vals, next_eq_vals = self.actor_t(in_seq=next_state_tensor)\n",
        "        next_vq_val = next_vq_vals.gather(index=next_v_id, dim=1)\n",
        "        trg_vq_val = reward_tensor.add(\n",
        "            self.gamma * next_vq_val.mul(done_tensor.logical_not())\n",
        "        )\n",
        "        vq_error = trg_vq_val.sub(pred_vq_val)\n",
        "        next_eq_val = next_eq_vals.gather(index=next_e_id, dim=1)\n",
        "        trg_eq_val = reward_tensor.add(\n",
        "            self.gamma * next_eq_val.mul(done_tensor.logical_not())\n",
        "        )\n",
        "        eq_error = trg_eq_val.sub(pred_eq_val)\n",
        "\n",
        "        actor_loss = self.a_criterion(vq_error=vq_error, eq_error=eq_error)\n",
        "        return actor_loss\n",
        "\n",
        "\n",
        "    def optimize_actor(self, actor_loss):\n",
        "        self.a_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm(\n",
        "            self.actor_m.parameters(), self.max_grad_norm\n",
        "        )\n",
        "        self.a_optimizer.step()\n",
        "        return\n",
        "\n",
        "    \n",
        "    def update_main_actor(self):\n",
        "        self.actor_m.train()\n",
        "        replay_batch = self.sample_experiences()\n",
        "        if replay_batch is None:\n",
        "            return\n",
        "        actor_loss = self.compute_actor_loss(replay_batch)\n",
        "        self.optimize_actor(actor_loss=actor_loss)\n",
        "        return\n",
        "\n",
        "\n",
        "    def train(self, train_env):\n",
        "        train_stats = {\n",
        "            \"avg_reward\": [],\n",
        "            \"avg_steps\" : [],\n",
        "            \"avg_wins\"  : []\n",
        "        }\n",
        "\n",
        "        # Initialize empty list for storing episode info at \n",
        "        # the start of every epoch.\n",
        "        e_rewards = []\n",
        "        e_steps   = []\n",
        "        e_wins    = []\n",
        "\n",
        "        for episode in range(self.nb_episodes):\n",
        "            # Reset at the beginning of every episode.\n",
        "            obs, _ = train_env.reset()\n",
        "            prev_score = 0\n",
        "            cum_sum_reward = 0\n",
        "            nb_steps = 0\n",
        "            done = False\n",
        "            won = False\n",
        "            cur_state_text = \"<sos> \" + obs + \" <eos>\"\n",
        "            cur_state_text = self.clean_text(text=cur_state_text)\n",
        "            cur_state_tensor = self.make_tensor(text=cur_state_text)\n",
        "            \n",
        "            while not done:\n",
        "                # Play one step using RL agent.\n",
        "                command, action_ids = self.get_train_command(in_seq=cur_state_tensor)\n",
        "                v_id = action_ids.get(\"v_id\", None)\n",
        "                e_id = action_ids.get(\"e_id\", None)\n",
        "                obs, score, done, _, info = train_env.step(command)\n",
        "                next_state_text = \"\"\n",
        "                obs = self.clean_text(obs)\n",
        "                info[\"description\"] = self.clean_text(info[\"description\"])\n",
        "                info[\"last_command\"] = self.clean_text(info[\"last_command\"])\n",
        "                if obs == info[\"description\"]:\n",
        "                    next_state_text = \"<sos> \" + info[\"last_command\"] + \" <eos> \" + \\\n",
        "                        \"<sos> \" + info[\"description\"] + \" <eos>\"\n",
        "                else:\n",
        "                    next_state_text = \"<sos> \" + info[\"last_command\"] + \" <eos> \" + \\\n",
        "                        \"<sos> \" + obs + \" <eos> \" + \\\n",
        "                        \"<sos> \" + info[\"description\"] + \" <eos>\"\n",
        "                next_state_text = self.clean_text(next_state_text)\n",
        "                next_state_tensor = self.make_tensor(next_state_text)\n",
        "                reward = score - prev_score\n",
        "                cum_sum_reward += reward\n",
        "                nb_steps += 1\n",
        "                won = info[\"won\"]\n",
        "\n",
        "                # Save the experience.\n",
        "                experience = (\n",
        "                    cur_state_tensor.detach().cpu().view(-1),\n",
        "                    v_id, e_id, reward,\n",
        "                    next_state_tensor.detach().cpu().view(-1),\n",
        "                    done\n",
        "                )\n",
        "                if reward > self.alpha_threshold:\n",
        "                    self.save_experience(experience, is_alpha=True)\n",
        "                else:\n",
        "                    self.save_experience(experience, is_alpha=False)\n",
        "\n",
        "                # Sample batch from replay biffer and update\n",
        "                # main actor parameters.\n",
        "                self.update_main_actor()\n",
        "\n",
        "                # Update cur_state and prev_score if not reached\n",
        "                # max_episode_steps.\n",
        "                if nb_steps >= self.max_episode_steps:\n",
        "                    break\n",
        "                cur_state_tensor = next_state_tensor\n",
        "                prev_score = score\n",
        "\n",
        "            e_rewards.append(cum_sum_reward) \n",
        "            e_steps.append(nb_steps)\n",
        "            e_wins.append(won)\n",
        "\n",
        "        train_stats[\"avg_reward\"] = np.nanmean(e_rewards) \n",
        "        train_stats[\"avg_steps\"] = np.nanmean(e_steps)\n",
        "        train_stats[\"avg_wins\"] = np.nanmean(e_wins)\n",
        "        return train_stats\n",
        "\n",
        "\n",
        "    def get_test_command(self, in_seq):\n",
        "        \"\"\"\n",
        "        This method returns the command that should be issued\n",
        "        by the RL agent in the testing phase.\n",
        "        \"\"\"\n",
        "        self.actor_m.eval()\n",
        "        batch_size = in_seq.shape[0]\n",
        "        command = \"\" \n",
        "        action_ids = {\"v_id\": 0, \"e_id\": 0}\n",
        "        with torch.no_grad():\n",
        "            vq_vals, eq_vals = self.actor_m(in_seq=in_seq)\n",
        "            _, v_id = vq_vals.topk(k=1, dim=1)\n",
        "            _, e_id = eq_vals.topk(k=1, dim=1)\n",
        "            # Select verb\n",
        "            verb = self.id2verb.get(v_id.item(), None)\n",
        "            # Select entity\n",
        "            entity = self.id2entity.get(e_id.item(), None)\n",
        "            command = f\"{verb} {entity}\"\n",
        "            action_ids[\"v_id\"] = v_id\n",
        "            action_ids[\"e_id\"] = e_id\n",
        "        return command, action_ids\n",
        "\n",
        "    \n",
        "    def test(self, test_env):\n",
        "        test_stats = {\n",
        "            \"avg_reward\": [],\n",
        "            \"avg_steps\" : [],\n",
        "            \"avg_wins\"  : []\n",
        "        }\n",
        "\n",
        "        # Initialize empty list for storing episode info at \n",
        "        # the start of every epoch.\n",
        "        e_rewards = []\n",
        "        e_steps   = []\n",
        "        e_wins    = []\n",
        "\n",
        "        for episode in range(self.nb_episodes):\n",
        "            # Reset at the beginning of every episode.\n",
        "            obs, _ = test_env.reset()\n",
        "            prev_score = 0\n",
        "            cum_sum_reward = 0\n",
        "            nb_steps = 0\n",
        "            done = False\n",
        "            won = False\n",
        "            cur_state_text = \"<sos> \" + obs + \" <eos>\"\n",
        "            cur_state_text = self.clean_text(text=cur_state_text)\n",
        "            cur_state_tensor = self.make_tensor(text=cur_state_text)\n",
        "\n",
        "            while not done:\n",
        "                # Play one step using RL agent.\n",
        "                command, action_ids = self.get_test_command(in_seq=cur_state_tensor)\n",
        "                v_id = action_ids.get(\"v_id\", None)\n",
        "                e_id = action_ids.get(\"e_id\", None)\n",
        "                obs, score, done, _, info = test_env.step(command)\n",
        "                next_state_text = \"\"\n",
        "                obs = self.clean_text(obs)\n",
        "                info[\"description\"] = self.clean_text(info[\"description\"])\n",
        "                info[\"last_command\"] = self.clean_text(info[\"last_command\"])\n",
        "                if obs == info[\"description\"]:\n",
        "                    next_state_text = \"<sos> \" + info[\"last_command\"] + \" <eos> \" + \\\n",
        "                        \"<sos> \" + info[\"description\"] + \" <eos>\"\n",
        "                else:\n",
        "                    next_state_text = \"<sos> \" + info[\"last_command\"] + \" <eos> \" + \\\n",
        "                        \"<sos> \" + obs + \" <eos> \" + \\\n",
        "                        \"<sos> \" + info[\"description\"] + \" <eos>\"\n",
        "                next_state_text = self.clean_text(next_state_text)\n",
        "                next_state_tensor = self.make_tensor(next_state_text)\n",
        "                reward = score - prev_score\n",
        "                cum_sum_reward += reward\n",
        "                nb_steps += 1\n",
        "                won = info[\"won\"]\n",
        "                if nb_steps >= self.max_episode_steps:\n",
        "                    break\n",
        "                cur_state_tensor = next_state_tensor\n",
        "                prev_score = score\n",
        "            \n",
        "            e_rewards.append(cum_sum_reward) \n",
        "            e_steps.append(nb_steps)\n",
        "            e_wins.append(won)\n",
        "\n",
        "        test_stats[\"avg_reward\"] = np.nanmean(e_rewards) \n",
        "        test_stats[\"avg_steps\"] = np.nanmean(e_steps)\n",
        "        test_stats[\"avg_wins\"] = np.nanmean(e_wins)\n",
        "        return test_stats\n",
        "\n",
        "\n",
        "    def play(self, train_env, test_env):\n",
        "        \"\"\"\n",
        "        Play the game using the agent.\n",
        "        \"\"\"\n",
        "        ep_stats = {\n",
        "            \"avg_reward_train\": [],\n",
        "            \"avg_steps_train\" : [],\n",
        "            \"avg_wins_train\"  : [],\n",
        "            \"avg_reward_test\" : [],\n",
        "            \"avg_steps_test\"  : [],\n",
        "            \"avg_wins_test\"   : []\n",
        "        }\n",
        "\n",
        "        for epoch in tqdm(range(self.epoch_offset+1, self.nb_epochs+1), unit=\"epoch\"):\n",
        "            start = time.time()\n",
        "            if epoch > 1:\n",
        "                self.epsilon = max(\n",
        "                    self.epsilon_end, self.epsilon - self.epsilon_decay_rate\n",
        "                )\n",
        "            ep_train_stats = self.train(train_env)\n",
        "            ep_test_stats = self.test(test_env)\n",
        "            if ((epoch == 1) or ((epoch % self.save_log_freq) == 0)):\n",
        "                msg = f\"\\nEpoch: {epoch}\\n\" + \\\n",
        "                    f\"Train avg. reward: {np.around(ep_train_stats['avg_reward'], 2)} \" + \\\n",
        "                    f\"Train avg. steps: {np.around(ep_train_stats['avg_steps'], 2)} \" + \\\n",
        "                    f\"Train avg. wins: {np.around(ep_train_stats['avg_wins'], 2)}\\n\" + \\\n",
        "                    f\"Test avg. reward: {np.around(ep_test_stats['avg_reward'], 2)} \" + \\\n",
        "                    f\"Test avg. steps: {np.around(ep_test_stats['avg_steps'], 2)} \" + \\\n",
        "                    f\"Test avg. wins: {np.around(ep_test_stats['avg_wins'], 2)}\\n\" + \\\n",
        "                    f\"Epsilon: {np.around(self.epsilon, 2)}\"\n",
        "                print(msg)\n",
        "                print(\"=\" * 75)\n",
        "\n",
        "            self.summary_writer.add_scalar(\"avg. reward\", ep_test_stats[\"avg_reward\"])\n",
        "            self.summary_writer.add_scalar(\"avg. steps\", ep_test_stats[\"avg_steps\"])\n",
        "            self.summary_writer.add_scalar(\"avg. wins\", ep_test_stats[\"avg_wins\"])\n",
        "\n",
        "            # Save checkpoint periodically.\n",
        "            if ((epoch == 1) or ((epoch % self.save_ckpt_freq) == 0)):\n",
        "                self.save_ckpt(epoch=epoch)\n",
        "            # Update target Actor network parameters periodically.\n",
        "            if ((epoch == 1) or ((epoch % self.t_update_freq) == 0)):\n",
        "                self.actor_t.load_state_dict(self.actor_m.state_dict())\n",
        "                \n",
        "            ep_stats[\"avg_reward_train\"].append(ep_train_stats[\"avg_reward\"])\n",
        "            ep_stats[\"avg_steps_train\"].append(ep_train_stats[\"avg_steps\"])\n",
        "            ep_stats[\"avg_wins_train\"].append(ep_train_stats[\"avg_wins\"])\n",
        "            ep_stats[\"avg_reward_test\"].append(ep_test_stats[\"avg_reward\"])\n",
        "            ep_stats[\"avg_steps_test\"].append(ep_test_stats[\"avg_steps\"])\n",
        "            ep_stats[\"avg_wins_test\"].append(ep_test_stats[\"avg_wins\"])\n",
        "            self.save_data(\n",
        "                data=ep_stats, file_name=\"ep_stats\", \n",
        "                directory=self.result_dir\n",
        "            )\n",
        "        return ep_stats"
      ],
      "metadata": {
        "id": "Ey4cMWvHsLRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test RL agent"
      ],
      "metadata": {
        "id": "VznwN0hUS5dL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set random generator seed for reproducibility."
      ],
      "metadata": {
        "id": "s_NJbYHZS5dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9cd88e0-5e94-44f6-a3fe-09cdc827f57d",
        "id": "N-CWTluzS5dM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f092f570f10>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get train and test environments, and Rl agent"
      ],
      "metadata": {
        "id": "C2wQUcYTS5dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "play_rl_agent = False  # Set to True if you want to play using Rl agent.\n",
        "if play_rl_agent:\n",
        "    config = Config()\n",
        "    train_twg = TextWorldGame(config=config)\n",
        "    rl_agent = RLAgent(\n",
        "        config=config,\n",
        "        vocab=train_twg.get_game_info().get(\"vocab\", None), \n",
        "        # verbs=[\"go\", \"take\"], # Pruned the verb space to simplify the problem\n",
        "        verbs=train_twg.get_game_controls().get(\"verbs\", None), \n",
        "        entities=train_twg.get_game_controls().get(\"entities\", None), \n",
        "    )\n",
        "    train_env = train_twg.get_game_info().get(\"env\", None)\n",
        "    test_twg = TextWorldGame(config=config)\n",
        "    test_env = test_twg.get_game_info().get(\"env\", None)"
      ],
      "metadata": {
        "id": "f1czRd1pS5dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test RL agent agent in training and test environment"
      ],
      "metadata": {
        "id": "3UVsGrfZS5dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if play_rl_agent:\n",
        "    rl_agent_stats = rl_agent.play(train_env=train_env, test_env=test_env)\n",
        "    rl_agent_stats_df = pd.DataFrame(rl_agent_stats)\n",
        "    rl_agent_stats_df[\"epoch\"] = np.arange(start=1, stop=rl_agent_stats_df.shape[0]+1)\n",
        "    rl_agent_stats_df"
      ],
      "metadata": {
        "id": "2t__4C-hS5dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if play_rl_agent:\n",
        "    plot_dir = f\"{config.plot_dir}/rl_agent-complex\"\n",
        "    if not os.path.isdir(plot_dir):\n",
        "        os.makedirs(plot_dir)\n",
        "\n",
        "    sns.lineplot(x=\"epoch\", y=\"avg_reward_test\", data=rl_agent_stats_df);\n",
        "    plt.savefig(f\"{plot_dir}/avg_reward_test.jpeg\");\n",
        "\n",
        "    sns.lineplot(x=\"epoch\", y=\"avg_steps_test\", data=rl_agent_stats_df);\n",
        "    plt.savefig(f\"{plot_dir}/avg_steps_test.jpeg\");\n",
        "\n",
        "    sns.lineplot(x=\"epoch\", y=\"avg_wins_test\", data=rl_agent_stats_df);\n",
        "    plt.savefig(f\"{plot_dir}/avg_wins_test.jpeg\");"
      ],
      "metadata": {
        "id": "pxn-SPj2DUYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QPTHZfJYigN1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "drl_for_tbg.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOhAAaooT/DBdMMttpVj2wU",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}