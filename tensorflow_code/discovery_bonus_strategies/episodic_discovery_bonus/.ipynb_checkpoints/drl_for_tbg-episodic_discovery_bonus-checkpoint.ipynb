{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzIvjaa3yAlI"
   },
   "source": [
    "## Install textworld learning environment ##\n",
    "Intsall the textorld ;learning environment. This will be used to create the various text-based games we are going to run our experiments on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "AHIL0ZWPCkG6",
    "outputId": "61bce85f-8b40-4936-f4e5-41712c407835"
   },
   "outputs": [],
   "source": [
    "!pip install textworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Owrk8hWO_PIX"
   },
   "source": [
    "## Create coin-collector game##\n",
    "Create a coin-collector game with 5 rooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "FG5y5QZVCYd6",
    "outputId": "793e8e95-bbfa-499d-936f-7b266576c0bc"
   },
   "outputs": [],
   "source": [
    "!tw-make tw-coin_collector --level 5 -f -v --seed 1997 --output \"tw_games/single_games/l5_easy/coin_collector_l5_easy.ulx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgyvUfnU9arT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Mapping, Any, Optional\n",
    "from collections import defaultdict, namedtuple, deque\n",
    "\n",
    "import time, datetime\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym \n",
    "import textworld.gym\n",
    "from textworld import EnvInfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aK5FTE1a9ar-"
   },
   "outputs": [],
   "source": [
    "class TextWorldGame:\n",
    "    \n",
    "    def __init__(self, game_config):\n",
    "        self.current_date_time = datetime.datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "        self.name  = f\"{game_config['name']}_\" + f\"created_on-{self.current_date_time}\"\n",
    "        self.path = game_config[\"path\"]\n",
    "        self.game_files = [self.path]\n",
    "        if os.path.isdir(self.path):\n",
    "            self.game_files = glob(os.path.join(path, \"*.ulx\"))\n",
    "            \n",
    "        self.max_steps = game_config[\"max_steps\"]\n",
    "        \n",
    "        self.env_id = textworld.gym.register_games(\n",
    "            self.game_files,\n",
    "            request_infos=self.request_infos,\n",
    "            max_episode_steps=self.max_steps\n",
    "        )\n",
    "        \n",
    "        self.env = gym.make(self.env_id)\n",
    "        \n",
    "        self.verbs = [\"go\", \"take\"]\n",
    "        self.nouns = [\"north\", \"east\", \"south\", \"west\", \"coin\"]\n",
    "        \n",
    "        self.game_info = {\n",
    "            \"env\" : self.env,\n",
    "            \"name\" : self.name,\n",
    "            \"max_steps\" : self.max_steps,\n",
    "            \"created_on\" : self.current_date_time\n",
    "        }\n",
    "        \n",
    "        self.game_controls = {\n",
    "            \"verbs\" : self.verbs,\n",
    "            \"nouns\" : self.nouns,\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        msg = f\"GAME: {self.game_info['name']}\\n\" + \\\n",
    "            \"====================================\\n\" + \\\n",
    "            f\"MAX. STEP: {self.game_info['max_step']}\\n\\n\" + \\\n",
    "            f\"VERBS: {self.game_controls['verbs']}\\n\\n\" + \\\n",
    "            f\"NOUNS: {self.game_controls['nouns']}\\n\" \n",
    "        \n",
    "        return msg\n",
    "    \n",
    "        \n",
    "    def get_game_info(self):\n",
    "        \"Return dictionary containing gym env. info.\"\n",
    "        return self.game_info\n",
    "    \n",
    "    \n",
    "    def get_game_controls(self):\n",
    "        \"Return dictionary containing gym env. info.\"\n",
    "        return self.game_controls\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def request_infos(self) -> EnvInfos:\n",
    "        request_infos = EnvInfos()\n",
    "        request_infos.description = True\n",
    "        request_infos.won = True\n",
    "        request_infos.last_command = True\n",
    "        \n",
    "        return request_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HuTywr5t9asj"
   },
   "source": [
    "## Building the random agent baseline ##\n",
    "We build an agent that plays the game by selecting random commands. This agent will be treated as the baseine agent against which the performnace of the RL agent will be measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IrtnN3YX9asn"
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \n",
    "    def __init__(self, config):  \n",
    "        \n",
    "        init_config     = config[\"init\"]\n",
    "        testing_config  = config[\"testing\"] \n",
    "        \n",
    "        self.agent_name              = init_config[\"general\"][\"agent_name\"]\n",
    "        \n",
    "        self.seed                    = init_config[\"general\"][\"seed\"]        \n",
    "        self.epochs                  = init_config[\"general\"][\"epochs\"]\n",
    "        \n",
    "        self.log_dir                 = init_config[\"general\"][\"log_dir\"]\n",
    "        self.logging_frequency       = init_config[\"general\"][\"logging_frequency\"]\n",
    "        \n",
    "        self.verbose                 = init_config[\"general\"][\"verbose\"]\n",
    "        \n",
    "        self.verbs                   = [\"go\", \"take\"]        \n",
    "        self.nouns                   = [\"north\", \"east\", \"south\", \"west\", \"coin\"]\n",
    "        self.noun_map                = dict()\n",
    "        self.verb_map                = dict()\n",
    "        \n",
    "        self._build_general_modules()\n",
    "        self._build_testing_modules(testing_config)\n",
    "        \n",
    "        \n",
    "    def _build_general_modules(self):\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "        \n",
    "        # Build verb map.\n",
    "        for i, v in enumerate(self.verbs):\n",
    "            self.verb_map[i] = v\n",
    "        # Build noun map.\n",
    "        for i, n in enumerate(self.nouns):\n",
    "            self.noun_map[i] = n\n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _build_testing_modules(self, testing_config):\n",
    "        self.max_episodes_testing = testing_config[\"scheduling\"][\"max_episodes\"]\n",
    "        self.max_steps_testing    = testing_config[\"scheduling\"][\"max_steps\"]\n",
    "            \n",
    "        return\n",
    "            \n",
    "    \n",
    "    def _choose_command(self):\n",
    "        \"\"\"Choose any command at random.\"\"\"        \n",
    "        v_idx = self.rng.randint(0, len(self.verb_map))\n",
    "        n_idx = self.rng.randint(0, len(self.noun_map))\n",
    "                \n",
    "        command_verb = self.verb_map[v_idx]\n",
    "        command_noun = self.noun_map[n_idx]\n",
    "        command      = f\"{command_verb} {command_noun}\"\n",
    "\n",
    "        return v_idx, n_idx, command\n",
    "    \n",
    "    \n",
    "    def play(self, testing_game_info, training_game_info=None, training_enabled=False):\n",
    "        \"\"\"Play the game in the selected mode.\"\"\"\n",
    "        \n",
    "        testing_game_name  = testing_game_info[\"name\"]\n",
    "        testing_game_env   = testing_game_info[\"env\"]\n",
    "        \n",
    "        # We will only return the stats corresponding to the testing phase of the agent.\n",
    "        stats = {\n",
    "            \"agent_name\"         : self.agent_name,\n",
    "            \"testing_game_name\"  : testing_game_name,\n",
    "            \"epochs\"             : self.epochs,\n",
    "            \"epochs_avg_reward\"  : [],\n",
    "            \"epochs_avg_steps\"   : [],\n",
    "            \"epochs_avg_wins\"    : []\n",
    "        }\n",
    "        \n",
    "        testing_game_name = testing_game_info[\"name\"]\n",
    "        testing_game_env  = testing_game_info[\"env\"]\n",
    "        \n",
    "        # Create log dir., if not present.\n",
    "        if not os.path.isdir(f\"{self.log_dir}_{testing_game_name}\"):\n",
    "            os.makedirs(f\"{self.log_dir}_{testing_game_name}\")\n",
    "            \n",
    "        # Create summary writer for tensorboard.\n",
    "        self.summary_writer = \\\n",
    "            tf.summary.create_file_writer(logdir=f\"{self.log_dir}_{testing_game_name}\")\n",
    "            \n",
    "        for epoch in tqdm(range(self.epochs), ascii=True, unit=\" epoch\"):\n",
    "            epoch_testing_stats = dict()\n",
    "            \n",
    "            epoch_testing_stats = self._test(testing_game_env)\n",
    "            \n",
    "            if self.verbose:\n",
    "                if (not ((epoch+1) % self.logging_frequency)) or (epoch == 0):\n",
    "                    print(\"\")\n",
    "                    print(f\"Epoch: {epoch}\")\n",
    "                    print(\"=============================================\")\n",
    "                    \n",
    "                    print(\"Test stats\")\n",
    "                    print(\"--------------------------------------------\")\n",
    "                    print(f\"Avg. reward: {epoch_testing_stats['avg_reward']:>5.3f}\")\n",
    "                    print(f\"Avg. steps: {epoch_testing_stats['avg_reward']:>5.3f}\")\n",
    "                    print(f\"Avg. wins: {epoch_testing_stats['avg_steps']:>5.3f}\")                    \n",
    "            \n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. reward\", \n",
    "                  epoch_testing_stats['avg_reward'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. steps\", \n",
    "                  epoch_testing_stats['avg_steps'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. wins\", \n",
    "                  epoch_testing_stats['avg_wins'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                \n",
    "            stats[\"epochs_avg_reward\"].append(epoch_testing_stats['avg_reward'])\n",
    "            stats[\"epochs_avg_steps\"].append(epoch_testing_stats['avg_steps'])\n",
    "            stats[\"epochs_avg_wins\"].append(epoch_testing_stats['avg_wins'])\n",
    "            \n",
    "        return stats\n",
    "    \n",
    "    \n",
    "    def _test(self, testing_game_env):\n",
    "        self.mode = \"testing\"\n",
    "        \n",
    "        testing_stats = {\n",
    "            \"avg_reward\" : 0,\n",
    "            \"avg_steps\" : 0,\n",
    "            \"avg_wins\" : 0\n",
    "        }        \n",
    "        \n",
    "        # Initialize empty list for storing episode info at the start of evry epoch.\n",
    "        episodes_rewards = []\n",
    "        episodes_steps   = []\n",
    "        episodes_won     = []\n",
    "        \n",
    "        for episode in range(self.max_episodes_testing):\n",
    "            # Reset environment at the start of evry episode.\n",
    "            obs, infos        =  testing_game_env.reset()\n",
    "            \n",
    "            cumulative_reward = 0\n",
    "            nb_moves          = 0  \n",
    "            done              = False\n",
    "            won               = False\n",
    "            \n",
    "            while not done:\n",
    "\n",
    "                v_idx, n_idx, command = self._choose_command()\n",
    "\n",
    "                obs, score, done, infos = testing_game_env.step(command)\n",
    "                \n",
    "                # Calculate reward based on score.\n",
    "                reward = score # Reward is equal to the score obtained after each move.\n",
    "\n",
    "                cumulative_reward += reward # Update cumulative reward.\n",
    "                nb_moves          += 1 # Update no. of moves (steps) played.\n",
    "                won               = infos[\"won\"]\n",
    "                \n",
    "                # Break out from while loop (i.e., end episode) if reached max. steps allowed.\n",
    "                if nb_moves >= self.max_steps_testing:\n",
    "                    break\n",
    "                    \n",
    "            episodes_rewards.append(cumulative_reward) # Rewards obtained every episode.\n",
    "            episodes_steps.append(nb_moves) # Steps played every episode.\n",
    "            episodes_won.append(won) # Info. regarding whether won episode or not.\n",
    "                \n",
    "        testing_stats[\"avg_reward\"] = np.mean(episodes_rewards)\n",
    "        testing_stats[\"avg_steps\"]  = np.mean(episodes_steps)\n",
    "        testing_stats[\"avg_wins\"]   = np.count_nonzero(episodes_won)/len(episodes_won)\n",
    "        \n",
    "        return testing_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taTFHeFC9auF"
   },
   "source": [
    "## Building the reinforcement learning agent\n",
    "Now, we build an RL agent which uses a LSTM-DQN model to issue the commands. This agent will have to be trained first before testing it so as to measure its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3ifD0Ur9auI"
   },
   "source": [
    "### Build the LSTM-DQN model ###\n",
    "First, create the LSTM-DQN model used by the RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rAWKgB7BJB5f"
   },
   "outputs": [],
   "source": [
    "class RepresentationGenerator(tf.keras.layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super(RepresentationGenerator, self).__init__()\n",
    "        self.seed = config[\"seed\"]\n",
    "        tf.random.set_seed(self.seed)\n",
    "        self.embedding_layer_input_dim = config[\"embedding_layer_input_dim\"]\n",
    "        self.embedding_layer_hidden_dim = config[\"embedding_layer_hidden_dim\"]\n",
    "        self.lstm_layer_hidden_dim = config[\"lstm_layer_hidden_dim\"]\n",
    "        self._build_layers()\n",
    "        \n",
    "    def _build_layers(self):\n",
    "        \n",
    "        self.embedding_layer = tf.keras.layers.Embedding(\n",
    "            self.embedding_layer_input_dim,\n",
    "            self.embedding_layer_hidden_dim,\n",
    "            name=\"embedding_layer\"\n",
    "        )        \n",
    "        self.lstm_layer = tf.keras.layers.LSTM(\n",
    "            self.lstm_layer_hidden_dim, \n",
    "            return_sequences=True,\n",
    "            name=\"lstm_layer\"\n",
    "        )        \n",
    "        self.mean_pooling_layer = tf.keras.layers.AveragePooling1D(\n",
    "            name=\"mean_pooling_layer\"\n",
    "        )\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding_layer(x)        \n",
    "        x = self.lstm_layer(x)        \n",
    "        x = self.mean_pooling_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8GIuYROJBsu"
   },
   "outputs": [],
   "source": [
    "class ActionScorer(tf.keras.layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super(ActionScorer, self).__init__()\n",
    "        self.seed = config[\"seed\"]\n",
    "        tf.random.set_seed(self.seed)\n",
    "        self.shared_mlp_layer_hidden_dim = config[\"shared_mlp_layer_hidden_dim\"]\n",
    "        self.verb_scorer_perceptron_hidden_dim = config[\"verb_scorer_perceptron_hidden_dim\"]\n",
    "        self.noun_scorer_perceptron_hidden_dim = config[\"noun_scorer_perceptron_hidden_dim\"]\n",
    "        self.verb_scorer_output_dim = config[\"verb_scorer_output_dim\"]\n",
    "        self.noun_scorer_output_dim = config[\"noun_scorer_output_dim\"]\n",
    "        self._build_layers()\n",
    "        \n",
    "    def _build_layers(self):\n",
    "        self.shared_mlp_layer = tf.keras.layers.Dense(\n",
    "            self.shared_mlp_layer_hidden_dim, \n",
    "            activation=\"relu\",\n",
    "            name=\"shared_mlp_layer\"\n",
    "        )        \n",
    "        self.verb_scorer_perceptron_layer = tf.keras.layers.Dense(\n",
    "            self.verb_scorer_perceptron_hidden_dim,\n",
    "            activation=\"relu\",\n",
    "            name=\"verb_scorer_perceptron_layer\"\n",
    "        )        \n",
    "        self.noun_scorer_perceptron_layer = tf.keras.layers.Dense(\n",
    "            self.noun_scorer_perceptron_hidden_dim,\n",
    "            activation=\"relu\",\n",
    "            name=\"noun_scorer_perceptron_layer\"\n",
    "        )\n",
    "        self.verb_scorer_layer = tf.keras.layers.Dense(\n",
    "            self.verb_scorer_output_dim, \n",
    "            name=\"verb_scores\"\n",
    "        )        \n",
    "        self.noun_scorer_layer = tf.keras.layers.Dense(\n",
    "            self.noun_scorer_output_dim, \n",
    "            name=\"noun_scores\"\n",
    "        )\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.shared_mlp_layer(x)\n",
    "        \n",
    "        x1 = self.verb_scorer_perceptron_layer(x)\n",
    "        x1 = self.verb_scorer_layer(x1)\n",
    "        \n",
    "        x2 = self.noun_scorer_perceptron_layer(x)\n",
    "        x2 = self.noun_scorer_layer(x2)\n",
    "        \n",
    "        return [x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zklj2v_E9auK"
   },
   "outputs": [],
   "source": [
    "class LSTM_DQN(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super(LSTM_DQN, self).__init__()\n",
    "        self.seed = config[\"general\"][\"seed\"]\n",
    "        tf.random.set_seed(self.seed)\n",
    "        self.representation_generator_config = {\n",
    "            \"seed\"                       : config[\"general\"][\"seed\"],\n",
    "            \"embedding_layer_input_dim\" : config[\"layers\"][\"embedding_input_dim\"],\n",
    "            \"embedding_layer_hidden_dim\" : config[\"layers\"][\"embedding_hidden_dim\"],\n",
    "            \"lstm_layer_hidden_dim\"      : config[\"layers\"][\"lstm_hidden_dim\"]\n",
    "        }\n",
    "        \n",
    "        self.action_scorer_config = {\n",
    "            \"seed\"                              : config[\"general\"][\"seed\"],\n",
    "            \"shared_mlp_layer_hidden_dim\"       : config[\"layers\"][\"shared_mlp_hidden_dim\"],\n",
    "            \"verb_scorer_perceptron_hidden_dim\" : config[\"layers\"][\"verb_scorer_perceptron_hidden_dim\"],\n",
    "            \"noun_scorer_perceptron_hidden_dim\" : config[\"layers\"][\"noun_scorer_perceptron_hidden_dim\"],\n",
    "            \"verb_scorer_output_dim\"            : config[\"layers\"][\"verb_scorer_output_dim\"],\n",
    "            \"noun_scorer_output_dim\"            : config[\"layers\"][\"noun_scorer_output_dim\"]        \n",
    "        }\n",
    "        \n",
    "        self._build_layers()\n",
    "        \n",
    "        \n",
    "    def _build_layers(self):\n",
    "        self.representation_generator = RepresentationGenerator(self.representation_generator_config)\n",
    "        self.action_scorer = ActionScorer(self.action_scorer_config)\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.representation_generator(x)\n",
    "        x1, x2 = self.action_scorer(x)\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uv7zUsDS9auY"
   },
   "source": [
    "### Transition Tuple ###\n",
    "Create a _nametuple_ called *Transitions* to store the experiences in the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSo_v9Us9aub"
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    \"Transition\",\n",
    "    (\"state\", \"v_idx\", \"n_idx\", \"reward\",\n",
    "     \"discovery_bonus\", \"next_state\", \"done\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYq0y_1o9aul"
   },
   "source": [
    "### Replay Memory###\n",
    "This is a vanilla replay memory used to store the experiences such that the stored experiences can be used later on for training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kk-x_YZj9aun"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, seed, capacity=100_000):\n",
    "        # vanilla replay memory\n",
    "        self.seed = seed\n",
    "        random.seed(self.seed)\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, minibatch_size):\n",
    "        return random.sample(self.memory, minibatch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owkf7QM19au1"
   },
   "source": [
    "### Prioritized Replay Memory Class ###\n",
    "We now create a prioritized replay memory to store the experiences so that they can be used later on for training the agent. Using the vanilla replay memory proved to be little disadvanatgeous due to the problem of **catatrophic forgetting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mQ8GGrm9au4"
   },
   "outputs": [],
   "source": [
    "class PrioritizedReplayMemory(object):\n",
    "\n",
    "    def __init__(self, seed, capacity=100_000, priority_fraction=0.0):\n",
    "        # prioritized replay memory\n",
    "        self.seed = seed\n",
    "        random.seed(self.seed)\n",
    "        self.priority_fraction = priority_fraction\n",
    "        self.alpha_capacity = int(capacity * priority_fraction)\n",
    "        self.beta_capacity = capacity - self.alpha_capacity\n",
    "        self.alpha_memory = []\n",
    "        self.beta_memory = []\n",
    "        self.alpha_position = 0 \n",
    "        self.beta_position = 0\n",
    "\n",
    "    def push(self, is_prior=False, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if is_prior:\n",
    "            if len(self.alpha_memory) < self.alpha_capacity:\n",
    "                self.alpha_memory.append(None)\n",
    "            self.alpha_memory[self.alpha_position] = Transition(*args)\n",
    "            self.alpha_position = (self.alpha_position + 1) % self.alpha_capacity\n",
    "        else:\n",
    "            if len(self.beta_memory) < self.beta_capacity:\n",
    "                self.beta_memory.append(None)\n",
    "            self.beta_memory[self.beta_position] = Transition(*args)\n",
    "            self.beta_position = (self.beta_position + 1) % self.beta_capacity\n",
    "\n",
    "    def sample(self, minibatch_size):\n",
    "        from_alpha = min(int(self.priority_fraction * minibatch_size), len(self.alpha_memory))\n",
    "        from_beta = min(minibatch_size - int(self.priority_fraction * minibatch_size), len(self.beta_memory))\n",
    "        res = random.sample(self.alpha_memory, from_alpha) + random.sample(self.beta_memory, from_beta)\n",
    "        random.shuffle(res)\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alpha_memory) + len(self.beta_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fzg_mleM9avE"
   },
   "source": [
    "### Reinforcement learning agent ###\n",
    "Now, we finally create the reinforcement learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LgIQHia9avH"
   },
   "outputs": [],
   "source": [
    "class RLAgent: \n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        init_config     = config[\"init\"]\n",
    "        training_config = config[\"training\"]\n",
    "        testing_config  = config[\"testing\"] \n",
    "        \n",
    "        self.agent_name              = init_config[\"general\"][\"agent_name\"]\n",
    "        \n",
    "        self.seed                    = init_config[\"general\"][\"seed\"]\n",
    "        self.max_vocab_size          = init_config[\"general\"][\"max_vocab_size\"]\n",
    "        self.epochs                  = init_config[\"general\"][\"epochs\"]\n",
    "        self.log_dir                 = init_config[\"general\"][\"log_dir\"]\n",
    "        self.logging_frequency       = init_config[\"general\"][\"logging_frequency\"]\n",
    "        \n",
    "        self.model_dir               = init_config[\"general\"][\"model_dir\"]\n",
    "        self.model_saving_frequency  = init_config[\"general\"][\"model_saving_frequency\"]\n",
    "        \n",
    "        self.verbose                 = init_config[\"general\"][\"verbose\"]\n",
    "        self.model_config            = init_config[\"model\"][\"lstm_dqn\"]\n",
    "        self.load_saved_model        = self.model_config[\"general\"][\"load_saved_model\"]\n",
    "        self.learning_rate           = self.model_config[\"optimizer\"][\"learning_rate\"]\n",
    "        \n",
    "        self.id2word                 = [\"<PAD>\", \"<UNK>\"]\n",
    "        self.word2id                 = {w: i for i, w in enumerate(self.id2word)}\n",
    "        \n",
    "        self.verbs                   = [\"go\", \"take\"]        \n",
    "        self.nouns                   = [\"north\", \"east\", \"south\", \"west\", \"coin\"]\n",
    "        self.noun_map                = dict()\n",
    "        self.verb_map                = dict()\n",
    "        \n",
    "        self._build_general_modules()\n",
    "        self._build_training_modules(training_config)\n",
    "        self._build_testing_modules(testing_config)\n",
    "\n",
    "        if (self.load_saved_model == True):\n",
    "            self._copy_weights(self.model_config[\"general\"][\"saved_model_path\"])\n",
    "        \n",
    "    \n",
    "    def _build_general_modules(self): \n",
    "        \n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "        \n",
    "        # Build verb map.\n",
    "        for i, v in enumerate(self.verbs):\n",
    "            self.verb_map[i] = v\n",
    "            \n",
    "        # Build noun map.\n",
    "        for i, n in enumerate(self.nouns):\n",
    "            self.noun_map[i] = n\n",
    "        \n",
    "        # Add the information abot number of verbs and nouns to\n",
    "        # model config dicitionary.\n",
    "        self.model_config[\"layers\"][\"verb_scorer_output_dim\"] = len(self.verb_map)\n",
    "        self.model_config[\"layers\"][\"noun_scorer_output_dim\"] = len(self.noun_map)        \n",
    "        \n",
    "        # Create new main lstm-dqn model.\n",
    "        self.main_model = self._create_model(self.model_config)\n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def _create_model(self, model_name):\n",
    "        \"\"\"Method to create the LSTM-DQN model.\"\"\"\n",
    "        # Create the model.\n",
    "        lstm_dqn = LSTM_DQN(self.model_config)\n",
    "        \n",
    "        # Compile the model.\n",
    "        lstm_dqn.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(self.learning_rate),\n",
    "            loss=tf.keras.losses.MSE,\n",
    "            metrics=\"accuracy\"\n",
    "        )\n",
    "    \n",
    "        return lstm_dqn\n",
    "    \n",
    "    \n",
    "    def _build_training_modules(self, training_config):        \n",
    "        self.max_episodes_training    = training_config[\"scheduling\"][\"max_episodes\"]\n",
    "        self.max_steps_training       = training_config[\"scheduling\"][\"max_steps\"]\n",
    "        self.bonus_coefficient        = training_config[\"scheduling\"][\"bonus_coefficient\"]\n",
    "        self.discount                 = training_config[\"scheduling\"][\"discount\"]\n",
    "        \n",
    "        self.replay_frequency         = training_config[\"scheduling\"][\"replay_frequency\"]\n",
    "        self.replay_counter           = 0\n",
    "        self.minibatch_size           = training_config[\"scheduling\"][\"minibatch_size\"]\n",
    "        self.target_update_frequency  = training_config[\"scheduling\"][\"target_update_frequency\"]\n",
    "        self.target_update_counter    = 0\n",
    "        self.min_replay_memory_size   = training_config[\"scheduling\"][\"min_replay_memory_size\"]\n",
    "        \n",
    "        self.replay_memory_capacity   = training_config[\"replay_memory\"][\"capacity\"]\n",
    "        self.replay_priority_fraction = training_config[\"replay_memory\"][\"priority_fraction\"]\n",
    "        \n",
    "        self.epsilon_init_training    = training_config[\"exploration\"][\"epsilon_init\"]\n",
    "        self.epsilon_min_training     = training_config[\"exploration\"][\"epsilon_min\"]\n",
    "        self.epsilon_decay_training   = training_config[\"exploration\"][\"epsilon_decay_rate\"]\n",
    "        self.epsilon_training         = self.epsilon_init_training\n",
    "        \n",
    "        # Target neural n/w model.\n",
    "        self.target_model = self._create_model(self.model_config)\n",
    "        self.target_model.set_weights(self.main_model.get_weights())\n",
    "        \n",
    "        # Replay memory.\n",
    "        self.replay_memory = PrioritizedReplayMemory(\n",
    "            seed=self.seed, \n",
    "            capacity=self.replay_memory_capacity, \n",
    "            priority_fraction=self.replay_priority_fraction\n",
    "        )\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _build_testing_modules(self, testing_config):\n",
    "        self.max_episodes_testing = testing_config[\"scheduling\"][\"max_episodes\"]\n",
    "        self.max_steps_testing    = testing_config[\"scheduling\"][\"max_steps\"]\n",
    "        self.epsilon_testing      = testing_config[\"exploration\"][\"epsilon_init\"]\n",
    "            \n",
    "        return\n",
    "\n",
    "\n",
    "    def _copy_weights(self, saved_model_path):\n",
    "        \"\"\"Copy weights from saved model to main model and target model.\"\"\"\n",
    "        reconstructed_model = tf.keras.models.load_model(saved_model_path)\n",
    "        \n",
    "        # Call the main model once to create the weights.\n",
    "        # This is necessary because otherwise we won't be able to copy\n",
    "        # weights.\n",
    "        self.main_model(tf.ones((1, 1997)))\n",
    "        # Copy weights from the representation generator layer of\n",
    "        # the reconstructred model to that of the main  model \n",
    "        # of the newly created RL agent.\n",
    "        self.main_model.set_weights(\n",
    "            reconstructed_model.get_weights()\n",
    "        )        \n",
    "        # Confirm that weights have been successfully copied.\n",
    "        assert len(self.main_model.weights) == len(reconstructed_model.weights)\n",
    "        for a, b in zip(self.main_model.weights, reconstructed_model.weights):\n",
    "            np.testing.assert_allclose(a.numpy(), b.numpy())\n",
    "        \n",
    "        # Call the target model once to create the weights.\n",
    "        # This is necessary because otherwise we won't be able to copy\n",
    "        # weights.\n",
    "        self.target_model(tf.ones((1, 1997)))\n",
    "        # Copy all the weights from main model\n",
    "        # to the target model.\n",
    "        self.target_model.set_weights(\n",
    "            self.main_model.get_weights()\n",
    "        ) \n",
    "        # Confirm that weights have been successfully copied.\n",
    "        assert len(self.target_model.weights) == len(self.main_model.weights)\n",
    "        for a, b in zip(self.target_model.weights, self.main_model.weights):\n",
    "            np.testing.assert_allclose(a.numpy(), b.numpy())\n",
    "            \n",
    "        return\n",
    "\n",
    "        \n",
    "    def _choose_command(self, state, eps):\n",
    "        \"\"\"Choose command using e-greedy strategy.\"\"\"\n",
    "        if self.rng.random() >= eps:\n",
    "            # Exploit.\n",
    "            v_idx, n_idx = self._get_max_qs_idx(self.main_model, [state])\n",
    "            v_idx = v_idx[0]\n",
    "            n_idx = n_idx[0]\n",
    "        else:\n",
    "            # Explore.\n",
    "            v_idx = self.rng.randint(0, len(self.verb_map))\n",
    "            n_idx = self.rng.randint(0, len(self.noun_map))\n",
    "                \n",
    "        command_verb = self.verb_map[v_idx]\n",
    "        command_noun = self.noun_map[n_idx]\n",
    "        command      = f\"{command_verb} {command_noun}\"\n",
    "\n",
    "        return v_idx, n_idx, command\n",
    "    \n",
    "    \n",
    "    def play(self, testing_game_info, training_game_info=None, training_enabled=False):\n",
    "        \"\"\"Play the game in the selected mode.\"\"\"\n",
    "        \n",
    "        if training_enabled == True:\n",
    "            training_game_name = training_game_info[\"name\"]\n",
    "            training_game_env  = training_game_info[\"env\"]\n",
    "        \n",
    "        testing_game_name  = testing_game_info[\"name\"]\n",
    "        testing_game_env   = testing_game_info[\"env\"]\n",
    "        \n",
    "         # Create log dir., if not present.\n",
    "        if not os.path.isdir(f\"{self.log_dir}_{testing_game_name}\"):\n",
    "            os.makedirs(f\"{self.log_dir}_{testing_game_name}\")\n",
    "            \n",
    "        # Create summary writer for tensorboard.\n",
    "        self.summary_writer = \\\n",
    "            tf.summary.create_file_writer(logdir=f\"{self.log_dir}_{testing_game_name}\")\n",
    "        \n",
    "        # Create model dir., if not present.\n",
    "        if not os.path.isdir(f\"{self.model_dir}_{testing_game_name}\"):\n",
    "            os.makedirs(f\"{self.model_dir}_{testing_game_name}\")\n",
    "        \n",
    "        # We will only return the stats c,orresponding to the testing phase of the agent.\n",
    "        stats = {\n",
    "            \"agent_name\"         : self.agent_name,\n",
    "            \"testing_game_name\"  : testing_game_name,\n",
    "            \"epochs\"             : self.epochs,\n",
    "            \"epochs_avg_reward\"  : [],\n",
    "            \"epochs_avg_steps\"   : [],\n",
    "            \"epochs_avg_wins\"    : []\n",
    "        }\n",
    "        \n",
    "        for epoch in tqdm(range(self.epochs), ascii=True, unit=\" epoch\"):\n",
    "            epoch_training_stats = dict()\n",
    "            epoch_testing_stats = dict()\n",
    "            \n",
    "            if training_enabled:\n",
    "                epoch_training_stats = self._train(training_game_env)\n",
    "                self.epsilon_training = max(self.epsilon_min_training, self.epsilon_decay_training * self.epsilon_training)\n",
    "\n",
    "            epoch_testing_stats = self._test(testing_game_env)\n",
    "            \n",
    "            if self.verbose:\n",
    "                if (not ((epoch+1) % self.logging_frequency)) or (epoch == 0):\n",
    "                    print(\"\")\n",
    "                    print(f\"Epoch: {epoch}\")\n",
    "                    print(\"=============================================\")\n",
    "                    \n",
    "                    if training_enabled:\n",
    "                        print(\"Train stats\")\n",
    "                        print(\"--------------------------------------------\")\n",
    "                        print(f\"Avg. reward: {epoch_training_stats['avg_reward']:>5.3f}\")\n",
    "                        print(f\"Avg. steps: {epoch_training_stats['avg_steps']:>5.3f}\")\n",
    "                        print(f\"Avg. wins: {epoch_training_stats['avg_wins']:>5.3f}\")\n",
    "                        print(f\"Epsilon: {self.epsilon_training:>5.3f}\")\n",
    "                    \n",
    "                    print(\"\")\n",
    "                    print(\"Test stats\")\n",
    "                    print(\"--------------------------------------------\")\n",
    "                    print(f\"Avg. reward: {epoch_testing_stats['avg_reward']:>5.3f}\")\n",
    "                    print(f\"Avg. steps: {epoch_testing_stats['avg_steps']:>5.3f}\")\n",
    "                    print(f\"Avg. wins: {epoch_testing_stats['avg_wins']:>5.3f}\")\n",
    "                    print(f\"Epsilon: {self.epsilon_testing:>5.3f}\")\n",
    "                    \n",
    "            \n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. reward\", \n",
    "                  epoch_testing_stats['avg_reward'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. steps\", \n",
    "                  epoch_testing_stats['avg_steps'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. wins\", \n",
    "                  epoch_testing_stats['avg_wins'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                \n",
    "            if training_enabled:\n",
    "                if (not ((epoch+1) % self.model_saving_frequency)):\n",
    "                    model_name = \\\n",
    "                        f\"epoch-{epoch}_\" + f\"avg_reward-{epoch_training_stats['avg_reward']}_\" + \\\n",
    "                        f\"avg_steps-{epoch_training_stats['avg_steps']}_\" + \\\n",
    "                        f\"avg_wins-{epoch_training_stats['avg_wins']}\" + \\\n",
    "                        f\"training_epsilon-{self.epsilon_training}\"\n",
    "                    # NOTE: Using the testing game name in the name of dir. where models are saved.\n",
    "                    model_out = os.path.join(f\"{self.model_dir}_{testing_game_name}\", model_name)\n",
    "                    self.main_model.save(model_out)\n",
    "                \n",
    "            stats[\"epochs_avg_reward\"].append(epoch_testing_stats['avg_reward'])\n",
    "            stats[\"epochs_avg_steps\"].append(epoch_testing_stats['avg_steps'])\n",
    "            stats[\"epochs_avg_wins\"].append(epoch_testing_stats['avg_wins'])\n",
    "\n",
    "        model_name = \\\n",
    "            f\"final_model_\" + \\\n",
    "            f\"epoch-{stats['epochs']-1}_\" + \\\n",
    "            f\"avg_reward-{stats['epochs_avg_reward'][-1]}_\" + \\\n",
    "            f\"avg_steps-{stats['epochs_avg_steps'][-1]}_\" + \\\n",
    "            f\"avg_wins-{stats['epochs_avg_wins'][-1]}\"\n",
    "        model_out = os.path.join(f\"{self.model_dir}_{testing_game_name}\", model_name)\n",
    "        self.main_model.save(model_out)\n",
    "            \n",
    "        return stats\n",
    "    \n",
    "    \n",
    "    def _train(self, training_game_env):\n",
    "        self.mode = \"training\"\n",
    "        \n",
    "        training_stats = {\n",
    "            \"avg_reward\" : 0,\n",
    "            \"avg_steps\" : 0,\n",
    "            \"avg_wins\" : 0\n",
    "        }        \n",
    "        \n",
    "        # Initialize empty list for storing episode info at the start of evry epoch.\n",
    "        episodes_rewards = []\n",
    "        episodes_steps   = []\n",
    "        episodes_won     = [] \n",
    "        \n",
    "        for episode in range(self.max_episodes_training):\n",
    "            # Reset state count dictionary to zero at the beginning of each epoch\n",
    "            # when employing episodic discovery strategy.\n",
    "            obs_counts = dict()\n",
    "        \n",
    "            # Reset environment at the start of evry episode.\n",
    "            obs, infos        = training_game_env.reset()\n",
    "\n",
    "            state             = f\"{obs}\\n{infos['description']}\\n{infos['last_command']}\"\n",
    "            \n",
    "            current_obs       = f\"{obs}\\n{infos['description']}\"\n",
    "            if not current_obs in obs_counts.keys(): \n",
    "                obs_counts[current_obs] = 1\n",
    "            else:\n",
    "                obs_counts[current_obs] += 1\n",
    "            \n",
    "            state_processed   = self._preprocess([state])\n",
    "            cumulative_reward = 0\n",
    "            nb_moves          = 0  \n",
    "            done              = False\n",
    "            won               = False\n",
    "            \n",
    "            while not done:\n",
    "\n",
    "                v_idx, n_idx, command = self._choose_command(state_processed, self.epsilon_training)\n",
    "\n",
    "                obs, score, done, infos = training_game_env.step(command)\n",
    "                \n",
    "                next_obs = f\"{obs}\\n{infos['description']}\"\n",
    "                if not next_obs in obs_counts.keys(): \n",
    "                    obs_counts[next_obs] = 1\n",
    "                else:\n",
    "                    obs_counts[next_obs] += 1\n",
    "\n",
    "                # Calculate reward based on score.\n",
    "                reward = score # Reward is equal to the score obtained after each move. \n",
    "                \n",
    "                # Calculate discovery bonus using episodic discovery strategy.    \n",
    "                discovery_bonus = self.bonus_coefficient if (obs_counts[current_obs]== 1) else 0\n",
    "\n",
    "                # Preprocess the next state.\n",
    "                next_state = f\"{obs}\\n{infos['description']}\\n{infos['last_command']}\"\n",
    "                next_state_processed = self._preprocess([next_state])                \n",
    "\n",
    "                # In training mode, we need to remember experiences and replay every few steps.\n",
    "                if reward >= 0.0:\n",
    "                    is_prior = True\n",
    "                else:\n",
    "                    is_prior = False\n",
    "                    \n",
    "                self._remember(\n",
    "                    is_prior, state_processed, v_idx, n_idx, reward, \n",
    "                    discovery_bonus, next_state_processed, done\n",
    "                )\n",
    "\n",
    "                #  After evry move, increment replay_counter.\n",
    "                self.replay_counter += 1\n",
    "                if self.replay_counter > self.replay_frequency: \n",
    "                    self._replay()\n",
    "                    self.replay_counter = 0\n",
    "                    \n",
    "                \n",
    "\n",
    "                state             = next_state\n",
    "                state_processed   = next_state_processed # Update current state.\n",
    "                current_obs       = next_obs\n",
    "                cumulative_reward += reward # Update cumulative reward.\n",
    "                nb_moves          += 1 # Update no. of moves (steps) played.\n",
    "                won               = infos[\"won\"]\n",
    "                \n",
    "                # self.epsilon_training = max(self.epsilon_min_training, self.epsilon_decay_training * self.epsilon_training)\n",
    "                \n",
    "                # Break out from while loop (i.e., end episode) if reached max. steps allowed.\n",
    "                if nb_moves >= self.max_steps_training:\n",
    "                    break\n",
    "                    \n",
    "            episodes_rewards.append(cumulative_reward) # Rewards obtained every episode.\n",
    "            episodes_steps.append(nb_moves) # Steps played every episode.\n",
    "            episodes_won.append(won) # Info. regarding whether won episode or not.\n",
    "\n",
    "            if self.mode == \"training\":\n",
    "                # If in training mode, increment target_update_counter at end of every episode.\n",
    "                self.target_update_counter += 1\n",
    "                if self.target_update_counter > self.target_update_frequency:\n",
    "                    self.target_model.set_weights(self.main_model.get_weights())\n",
    "                    self.target_update_counter = 0\n",
    "                \n",
    "        training_stats[\"avg_reward\"] = np.mean(episodes_rewards)\n",
    "        training_stats[\"avg_steps\"]  = np.mean(episodes_steps)\n",
    "        training_stats[\"avg_wins\"]   = np.count_nonzero(episodes_won)/len(episodes_won)\n",
    "        \n",
    "        return training_stats\n",
    "    \n",
    "    \n",
    "    def _test(self, testing_game_env):\n",
    "        self.mode = \"testing\"\n",
    "        \n",
    "        testing_stats = {\n",
    "            \"avg_reward\" : 0,\n",
    "            \"avg_steps\" : 0,\n",
    "            \"avg_wins\" : 0\n",
    "        }        \n",
    "        \n",
    "        # Initialize empty list for storing episode info at the start of evry epoch.\n",
    "        episodes_rewards = []\n",
    "        episodes_steps   = []\n",
    "        episodes_won     = []\n",
    "        \n",
    "        for episode in range(self.max_episodes_testing):\n",
    "            # Reset environment at the start of evry episode.\n",
    "            obs, infos        =  testing_game_env.reset()\n",
    "\n",
    "            state             = f\"{obs}\\n{infos['description']}\\n{infos['last_command']}\"\n",
    "            state_processed   = self._preprocess([state])\n",
    "            cumulative_reward = 0\n",
    "            nb_moves          = 0  \n",
    "            done              = False\n",
    "            won               = False\n",
    "            \n",
    "            while not done:\n",
    "\n",
    "                v_idx, n_idx, command = self._choose_command(state_processed, self.epsilon_testing)\n",
    "\n",
    "                obs, score, done, infos = testing_game_env.step(command)\n",
    "\n",
    "                # Calculate reward based on score.\n",
    "                reward = score # Reward is equal to the score obtained after each move.               \n",
    "\n",
    "                # Preprocess the next state.\n",
    "                next_state = f\"{obs}\\n{infos['description']}\\n{infos['last_command']}\"\n",
    "                next_state_processed = self._preprocess([next_state])\n",
    "\n",
    "                state             = next_state\n",
    "                state_processed   = next_state_processed # Update current state.\n",
    "                cumulative_reward += reward # Update cumulative reward.\n",
    "                nb_moves          += 1 # Update no. of moves (steps) played.\n",
    "                won               = infos[\"won\"]\n",
    "                \n",
    "                # Break out from while loop (i.e., end episode) if reached max. steps allowed.\n",
    "                if nb_moves >= self.max_steps_testing:\n",
    "                    break\n",
    "                    \n",
    "            episodes_rewards.append(cumulative_reward) # Rewards obtained every episode.\n",
    "            episodes_steps.append(nb_moves) # Steps played every episode.\n",
    "            episodes_won.append(won) # Info. regarding whether won episode or not.\n",
    "                \n",
    "        testing_stats[\"avg_reward\"] = np.mean(episodes_rewards)\n",
    "        testing_stats[\"avg_steps\"]  = np.mean(episodes_steps)\n",
    "        testing_stats[\"avg_wins\"]   = np.count_nonzero(episodes_won)/len(episodes_won)\n",
    "        \n",
    "        return testing_stats\n",
    "    \n",
    "    \n",
    "    def _remember(self, is_prior, state, v_idx, n_idx, reward, discovery_bonus, next_state, done):\n",
    "        \"\"\"Update replay memory.\"\"\"\n",
    "        self.replay_memory.push(is_prior, state, v_idx, n_idx, reward, discovery_bonus, next_state, done)\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def _get_max_qs_idx(self, nn_model, state_batch):\n",
    "        \"\"\"Get indexes corresponding to the max. verb and noun scores.\"\"\"\n",
    "        \n",
    "        # Add padding to the batch.\n",
    "        state_batch = self._add_padding(state_batch)\n",
    "        \n",
    "        verb_scores_batch, noun_scores_batch = \\\n",
    "            nn_model.predict(state_batch)\n",
    "        \n",
    "        max_verb_score_idx_batch = np.argmax(verb_scores_batch, axis=2)\n",
    "        max_verb_score_idx_batch = max_verb_score_idx_batch[:, 0]\n",
    "        \n",
    "        max_noun_score_idx_batch = np.argmax(noun_scores_batch, axis=2)\n",
    "        max_noun_score_idx_batch = max_noun_score_idx_batch[:, 0]\n",
    "        \n",
    "        return max_verb_score_idx_batch, max_noun_score_idx_batch\n",
    "    \n",
    "    \n",
    "    def _get_max_qs(self, nn_model, state_batch):\n",
    "        \"\"\"Get max. verb and noun scores.\"\"\"\n",
    "        \n",
    "        # Add padding to the batch.\n",
    "        state_batch = self._add_padding(state_batch)\n",
    "        \n",
    "        verb_scores_batch, noun_scores_batch = \\\n",
    "            nn_model.predict(state_batch)\n",
    "        \n",
    "        max_verb_score_batch = np.max(verb_scores_batch, axis=2)\n",
    "\n",
    "        max_verb_score_batch = max_verb_score_batch[:, 0]\n",
    "        \n",
    "        max_noun_score_batch = np.max(noun_scores_batch, axis=2)\n",
    "        max_noun_score_batch = max_noun_score_batch[:, 0]\n",
    "        \n",
    "        return max_verb_score_batch, max_noun_score_batch\n",
    "    \n",
    "    \n",
    "    def _get_qs(self, nn_model, state_batch):\n",
    "        \"\"\"Get list of verb scores and list of noun scores.\"\"\"\n",
    "        \n",
    "        # Add padding to the batch.\n",
    "        state_batch = self._add_padding(state_batch)\n",
    "        \n",
    "        verb_scores_batch, noun_scores_batch = \\\n",
    "            nn_model.predict(state_batch)\n",
    "        \n",
    "        return verb_scores_batch, noun_scores_batch\n",
    "    \n",
    "    \n",
    "    def _get_target_scores(self, idx_batch, scores_batch, target_batch):\n",
    "        \"\"\"\n",
    "        Create the batch of target scores (verb/noun) \n",
    "        used for fitting the model.\n",
    "        \"\"\"\n",
    "        scores_target_batch = []\n",
    "        \n",
    "        for idx, scores, target in zip(idx_batch, scores_batch, target_batch):\n",
    "            scores_target = np.copy(scores)\n",
    "            scores_target[0, idx] = target\n",
    "            scores_target_batch.append(scores_target)\n",
    "        \n",
    "        scores_target_batch = tf.constant(scores_target_batch)\n",
    "        \n",
    "        return scores_target_batch\n",
    "    \n",
    "    \n",
    "    def _replay(self):\n",
    "        \"\"\"Creat source-target pairs and perform gradient update. \"\"\"\n",
    "        \n",
    "        # Sample a minibatch of transitions from the \n",
    "        # experience replay memory and create req. lists\n",
    "        # to perform nececessary operations on.\n",
    "        \n",
    "        if len(self.replay_memory) < self.min_replay_memory_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = self.replay_memory.sample(self.minibatch_size)\n",
    "        \n",
    "        state_batch           = [data.state for data in minibatch]        \n",
    "        v_idx_batch           = [data.v_idx for data in minibatch]        \n",
    "        n_idx_batch           = [data.n_idx for data in minibatch]        \n",
    "        reward_batch          = [data.reward for data in minibatch]\n",
    "        discovery_bonus_batch = [data.discovery_bonus for data in minibatch]\n",
    "        next_state_batch      = [data.next_state for data in minibatch]        \n",
    "        done_batch            = [data.done for data in minibatch]\n",
    "        \n",
    "        # Get the batch of max. verb and noun scores corresponding\n",
    "        # to the next states.\n",
    "        max_verb_score_batch, max_noun_score_batch = \\\n",
    "            self._get_max_qs(self.target_model, next_state_batch)\n",
    "        \n",
    "        # Get the Q-learning targets for verb scores.\n",
    "        verb_target_batch = []\n",
    "        for done, reward, discovery_bonus, max_verb_score in \\\n",
    "        zip(done_batch, reward_batch, discovery_bonus_batch, max_verb_score_batch):\n",
    "            if done:\n",
    "                verb_target = (reward + discovery_bonus)\n",
    "            else:\n",
    "                verb_target = (reward + discovery_bonus) + (self.discount * max_verb_score)\n",
    "            \n",
    "            verb_target_batch.append(verb_target)\n",
    "        \n",
    "        # Get the Q-learning targets for noun scores.\n",
    "        noun_target_batch = []\n",
    "        for done, reward, discovery_bonus, max_noun_score in \\\n",
    "        zip(done_batch, reward_batch, discovery_bonus_batch, max_noun_score_batch):\n",
    "            if done:\n",
    "                noun_target = (reward + discovery_bonus)\n",
    "            else:\n",
    "                noun_target = (reward + discovery_bonus) + (self.discount * max_noun_score)\n",
    "            \n",
    "            noun_target_batch.append(noun_target)\n",
    "        \n",
    "        # Get the verb and noun scores corresponding to batch of \n",
    "        # current state.\n",
    "        verb_scores_batch, noun_scores_batch = \\\n",
    "            self._get_qs(self.main_model, state_batch)   \n",
    "        \n",
    "        # IMP: Create the batch of target verb scores.\n",
    "        verb_scores_target_batch = self._get_target_scores(\n",
    "            v_idx_batch, verb_scores_batch,\n",
    "            verb_target_batch\n",
    "        )\n",
    "        \n",
    "        # IMP: Create the batch of target verb scores.\n",
    "        noun_scores_target_batch = self._get_target_scores(\n",
    "            n_idx_batch, noun_scores_batch,\n",
    "            noun_target_batch\n",
    "        )        \n",
    "            \n",
    "        # IMP: Create the batch of source input sequence.\n",
    "        input_seq_data_batch = self._add_padding(state_batch)        \n",
    "\n",
    "        # Fit the main model.\n",
    "        self.main_model.fit(\n",
    "            input_seq_data_batch,\n",
    "            [verb_scores_target_batch, noun_scores_target_batch],\n",
    "            batch_size=self.minibatch_size,\n",
    "            verbose=False,\n",
    "            shuffle=False\n",
    "        )\n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _get_word_id(self, word):\n",
    "        if word not in self.word2id:\n",
    "            if len(self.word2id) >= self.max_vocab_size:\n",
    "                return self.word2id[\"<UNK>\"]\n",
    "            \n",
    "            self.id2word.append(word)\n",
    "            self.word2id[word] = len(self.word2id)\n",
    "            \n",
    "        return self.word2id[word]\n",
    "    \n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
    "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
    "        word_ids = list(map(self._get_word_id, text.split()))\n",
    "        return word_ids\n",
    "\n",
    "    \n",
    "    def _add_padding(self, texts):\n",
    "        \"\"\"Add padding to the batch of texts.\"\"\"\n",
    "        max_len = max(len(text[0]) for text in texts)\n",
    "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            padded[i, :len(text[0])] = text\n",
    "            \n",
    "        padded_texts = np.copy(padded)\n",
    "        return padded_texts\n",
    "    \n",
    "    \n",
    "    def _preprocess(self, texts):\n",
    "        \"\"\"Tokenize and padding (if req.) the text.\"\"\"\n",
    "        texts = list(map(self._tokenize, texts))\n",
    "        texts = np.array(texts)\n",
    "        \n",
    "        return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eF966QQ_9atI"
   },
   "source": [
    "## Method to evaluate performance of agent\n",
    "Next , we define a method to evaluate performance of a (**random/RL**) agent as it plays any (**text based**) game in (**train/test**) mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCwZuAHD9atO"
   },
   "outputs": [],
   "source": [
    "def evaluate_performance(agent, training_game_config, testing_game_config, training_enabled=False):\n",
    "    \n",
    "    # Create textworld object anf get req. info to play game.\n",
    "    text_world_game_testing = TextWorldGame(testing_game_config)\n",
    "    testing_game_info = text_world_game_testing.get_game_info()\n",
    "    \n",
    "    if training_enabled:\n",
    "        text_world_game_training = TextWorldGame(training_game_config)\n",
    "        training_game_info = text_world_game_training.get_game_info()\n",
    "    else:\n",
    "        training_game_info = None\n",
    "    \n",
    "    stats = agent.play(testing_game_info, training_game_info, training_enabled)\n",
    "        \n",
    "    # Stats containing info. related to the diff. epochs.    \n",
    "    agent_name        = stats[\"agent_name\"]\n",
    "    testing_game_name = stats[\"testing_game_name\"]\n",
    "    epochs            = stats[\"epochs\"]\n",
    "    epochs_avg_reward = stats[\"epochs_avg_reward\"]\n",
    "    epochs_avg_steps  = stats[\"epochs_avg_steps\"]\n",
    "    epochs_avg_wins   = stats[\"epochs_avg_wins\"]\n",
    "    \n",
    "    plt_path = os.path.join(\"plots\", f\"{agent_name}_{testing_game_name}\")\n",
    "    if not os.path.isdir(plt_path):\n",
    "        os.makedirs(plt_path)\n",
    "\n",
    "    stats_path = \"stats\"\n",
    "    if not os.path.isdir(stats_path):\n",
    "        os.makedirs(stats_path)\n",
    "    pickle_out = open(\n",
    "        f\"{stats_path}/{agent_name}_{testing_game_name}\", \n",
    "        \"wb\"\n",
    "    )\n",
    "    pickle.dump(stats, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    # Save the vocabulary.\n",
    "    vocab_path_root = \"vocabulary\"\n",
    "    vocab_path = os.path.join(vocab_path_root, f\"{agent_name}_{testing_game_name}\")\n",
    "    if not os.path.isdir(vocab_path):\n",
    "        os.makedirs(vocab_path)\n",
    "\n",
    "    # Save id2word.\n",
    "    pickle_out = open(\n",
    "        f\"{vocab_path}/{agent_name}_{testing_game_name}_id2word\", \n",
    "        \"wb\"\n",
    "    )\n",
    "    pickle.dump(agent.id2word, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "    # Save word2id.\n",
    "    pickle_out = open(\n",
    "        f\"{vocab_path}/{agent_name}_{testing_game_name}_word2id\", \n",
    "        \"wb\"\n",
    "    )\n",
    "    pickle.dump(agent.word2id, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    \"\"\"Plot the learning curves.\"\"\"\n",
    "    \n",
    "    # Fig 1: Avg. reward over all epochs.\n",
    "    fig = plt.figure(figsize=(10, 6))    \n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_title(\n",
    "        f\"GAME- {testing_game_name}\\n\" + \\\n",
    "        f\"========================================================\\n\" + \\\n",
    "        f\"Avg. rewards over {epochs} epochs\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Avg. reward\")\n",
    "    ax.plot(range(epochs), epochs_avg_reward)\n",
    "    plt.savefig(f\"{plt_path}/avg_rewards.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Fig 2: Avg. steps over all epochs.\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_title(\n",
    "        f\"GAME- {testing_game_name}\\n\" + \\\n",
    "        f\"========================================================\\n\" + \\\n",
    "        f\"Avg. steps over {epochs} epochs\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Avg. steps\")\n",
    "    ax.plot(range(epochs), epochs_avg_steps)\n",
    "    plt.savefig(f\"{plt_path}/avg_steps.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Fig 3: Avg. steps over all epochs.\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_title(\n",
    "        f\"GAME- {testing_game_name}\\n\" + \\\n",
    "        f\"========================================================\\n\" + \\\n",
    "        f\"Avg. wins over {epochs} epochs\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Avg. wins\")\n",
    "    ax.plot(range(epochs), epochs_avg_wins)\n",
    "    plt.savefig(f\"{plt_path}/avg_wins.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0peEwESP9ati"
   },
   "source": [
    "## Get games for evaluating agent performances##\n",
    "Here, we obtain the trainig games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jAd6Ks9CkX-w"
   },
   "outputs": [],
   "source": [
    "game_config = dict()\n",
    "game_config[\"name\"] = \"coin_collector_l10_easy\"\n",
    "game_config[\"max_steps\"] = 100\n",
    "game_config[\"path\"] = \"tw_games/single_games/l10_easy/coin_collector_l10_easy.ulx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RjY_hTFSkX-5"
   },
   "source": [
    "## Get that agents to play the game ##\n",
    "Now, we get the **Random Agent** and **RL Agent** to play the coin collectr game having 5 rooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LPS4ujWkX-7"
   },
   "source": [
    "### Get random agent ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8rU8SPPkX-7"
   },
   "outputs": [],
   "source": [
    "# Read random agent config details from config file.\n",
    "config_file = open(\"configs/random_agent_config.yaml\")\n",
    "random_agent_config = yaml.safe_load(config_file)\n",
    "config_file.close()\n",
    "\n",
    "random_agent = RandomAgent(random_agent_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9eVvJ0PkX_D"
   },
   "source": [
    "### Get RL agent ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1z2ldTQnkX_E"
   },
   "outputs": [],
   "source": [
    "# Read config details from config file.\n",
    "config_file = open(\"configs/rl_agent_config.yaml\")\n",
    "rl_agent_config = yaml.safe_load(config_file)\n",
    "config_file.close()\n",
    "\n",
    "print(rl_agent_config[\"training\"])\n",
    "\n",
    "rl_agent = RLAgent(rl_agent_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Qt-r8TRkX_L"
   },
   "source": [
    "## Test random agent on single games ##\n",
    "Test and evaluate the performance of the random agent on single games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HIp4mq8qkX_N",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_agent_testing_config = random_agent_config[\"testing\"]\n",
    "random_agent_testing_stats = evaluate_performance(\n",
    "    agent=random_agent, \n",
    "    testing_game_config=game_config,\n",
    "    training_game_config=None, \n",
    "    training_enabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ME1Jh9fmkX_S"
   },
   "source": [
    "## Test RL agent on single games ##\n",
    "Test and evaluate the performance of the RL agent on single games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QNkyPXwikX_V",
    "outputId": "2ec0d1c9-ce67-4feb-ebdb-0b9861d535a6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rl_agent_stats = evaluate_performance(\n",
    "    agent=rl_agent, \n",
    "    testing_game_config=game_config,\n",
    "    training_game_config=game_config,\n",
    "    training_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary.\n",
    "vocab_path_root = \"vocabulary\"\n",
    "vocab_path = os.path.join(vocab_path_root, f\"{rl_agent_stats['agent_name']}_{rl_agent_stats['testing_game_name']}\")\n",
    "if not os.path.isdir(vocab_path):\n",
    "    os.makedirs(vocab_path)\n",
    "\n",
    "# Save id2word.\n",
    "pickle_out = open(\n",
    "    f\"{vocab_path}/{rl_agent_stats['agent_name']}_{rl_agent_stats['testing_game_name']}_id2word\", \n",
    "    \"wb\"\n",
    ")\n",
    "pickle.dump(rl_agent.id2word, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# Save word2id.h\n",
    "pickle_out = open(\n",
    "    f\"{vocab_path}/{rl_agent_stats['agent_name']}_{rl_agent_stats['testing_game_name']}_word2id\", \n",
    "    \"wb\"\n",
    ")\n",
    "pickle.dump(rl_agent.word2id, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXDtB4ZLDF4G"
   },
   "source": [
    "### Reconstruct from previously saved model for transfer learning purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e47m53rNnFMY"
   },
   "outputs": [],
   "source": [
    "reconstructed_model = tf.keras.models.load_model(\n",
    "    \"/content/gdrive/My Drive/rl_nlp_research/text_world_based_games/coin_collector_game/3.episodic_discovery_bonus/models/\" + \\\n",
    "    \"rl_agent_coin_collector_l5_easy_created_on-09-03-2020_04-53-39/\" + \\\n",
    "    \"final_model_epoch-99_avg_reward-0.9_avg_steps-11.66_avg_wins-0.9_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCYdO2JwDMC9"
   },
   "source": [
    "### Create a new RL agent and set weights\n",
    "Here, we create a new RL agent and set the weights of the representation generator layer using that from the reconstructed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zelkFsxfDHG9"
   },
   "outputs": [],
   "source": [
    "# Read config details from config file.\n",
    "config_file = open(\"/content/gdrive/My Drive/rl_nlp_research/text_world_based_games/coin_collector_game/3.episodic_discovery_bonus/configs/rl_agent_config.yaml\")\n",
    "rl_agent_config = yaml.safe_load(config_file)\n",
    "config_file.close()\n",
    "\n",
    "rl_agent_new = RLAgent(rl_agent_config)\n",
    "# Call the main model once to create the weights.\n",
    "# This is necessary because otherwise we won't be able to copy\n",
    "# weights.\n",
    "rl_agent_new.main_model(tf.ones((1, 1997)))\n",
    "\n",
    "# Copy weights from the representation generator layer of\n",
    "# the reconstructred model to that of the main  model \n",
    "# of the newly created RL agent.\n",
    "rl_agent_new.main_model.representation_generator.set_weights(\n",
    "    reconstructed_model.representation_generator.get_weights()\n",
    ")\n",
    "\n",
    "# Confirm that weights have been successfully copied.\n",
    "assert len(rl_agent_new.main_model.representation_generator.weights) == len(reconstructed_model.representation_generator.weights)\n",
    "for a, b in zip(rl_agent_new.main_model.representation_generator.weights, reconstructed_model.representation_generator.weights):\n",
    "    np.testing.assert_allclose(a.numpy(), b.numpy())\n",
    "\n",
    "# Call the target model once to create the weights.\n",
    "# This is necessary because otherwise we won't be able to copy\n",
    "# weights.\n",
    "rl_agent_new.target_model(tf.ones((1, 1997)))\n",
    "\n",
    "# Copy all the weights from main model\n",
    "# to the target model.\n",
    "rl_agent_new.target_model.set_weights(\n",
    "    rl_agent_new.main_model.get_weights()\n",
    ")\n",
    "\n",
    "# Confirm that weights have been successfully copied.\n",
    "assert len(rl_agent_new.target_model.weights) == len(rl_agent_new.main_model.weights)\n",
    "for a, b in zip(rl_agent_new.target_model.weights, rl_agent_new.main_model.weights):\n",
    "    np.testing.assert_allclose(a.numpy(), b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4uw6HjQ8DWa0"
   },
   "source": [
    "### Train and test new RL agent on the unseen game ###\n",
    "Now, we will evaluate the performance of the newly created RL agent on the unseen game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOS6mYnKnrGu"
   },
   "source": [
    "### Create unseen game with same difficulty but different map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "9wTYqbYhn1R0",
    "outputId": "eb7ce514-8493-439d-fc5e-f5ecdd47d480"
   },
   "outputs": [],
   "source": [
    "!tw-make tw-coin_collector --level 5 -f -v --seed 1996 --output \"/content/gdrive/My Drive/rl_nlp_research/text_world_based_games/coin_collector_game/3.episodic_discovery_bonus/tw_games/single_games/l5_easy2/coin_collector_l5_easy2.ulx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAUUsCumDSox"
   },
   "outputs": [],
   "source": [
    "game_config2 = dict()\n",
    "game_config2[\"name\"] = \"coin_collector_l5_easy2\"\n",
    "game_config2[\"max_steps\"] = 50\n",
    "game_config2[\"path\"] = \"/content/gdrive/My Drive/rl_nlp_research/text_world_based_games/coin_collector_game/3.episodic_discovery_bonus/tw_games/single_games/l5_easy2/coin_collector_l5_easy2.ulx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LJLgxs5boXb8",
    "outputId": "161274a9-8bd0-485a-89a9-eb40ce2edc1c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rl_agent_new_stats = evaluate_performance(\n",
    "    agent=rl_agent_new, \n",
    "    testing_game_config=game_config2,\n",
    "    training_game_config=game_config2,\n",
    "    training_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9WQnFn2HojMD"
   },
   "source": [
    "### Create a new RL agent and play the same unseen game\n",
    "Here, we will create a new RL agent that will learn to play the game from scratch without copying weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8KAJg6JokaO"
   },
   "outputs": [],
   "source": [
    "# Read config details from config file.\n",
    "config_file = open(\"/content/gdrive/My Drive/rl_nlp_research/text_world_based_games/coin_collector_game/3.episodic_discovery_bonus/configs/rl_agent_config.yaml\")\n",
    "rl_agent_config = yaml.safe_load(config_file)\n",
    "config_file.close()\n",
    "\n",
    "rl_agent_new2 = RLAgent(rl_agent_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "h95qjQxgonhE",
    "outputId": "ea10649d-fde1-45d0-f038-211bb694f74d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rl_agent_new2_stats = evaluate_performance(\n",
    "    agent=rl_agent_new2, \n",
    "    testing_game_config=game_config2,\n",
    "    training_game_config=game_config2,\n",
    "    training_enabled=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TRANSFER_LEARNING_playing_game_with_random_and_rl_agent_episodic_discovery_bonus_using_model_subclassing_ACTIVE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
