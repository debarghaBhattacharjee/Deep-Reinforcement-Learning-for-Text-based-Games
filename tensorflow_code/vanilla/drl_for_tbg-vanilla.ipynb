{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Owrk8hWO_PIX"
   },
   "source": [
    "## Create coin-collector game##\n",
    "Create a coin-collector game with 5 rooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "FG5y5QZVCYd6",
    "outputId": "b79c2137-4db1-459e-b8f8-9babd063295c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed: 1996\n",
      "Game generated: /home/cs19s028/IIT Madras/Research/coin_collector_game_custom/coin_collector_game_tests/1.vanilla/tw_games/single_games/l5_easy_2/coin_collector_l5_easy_2.ulx\n",
      "\n",
      "Objective:\n",
      "I hope you're ready to go into rooms and interact with objects, because you've just entered TextWorld! Here is how to play! First off, move east. With that accomplished, attempt to venture north. Okay, and then, make an effort to move east. With that accomplished, make an attempt to head north. After that, pick-up the coin from the floor of the chamber. Once that's all handled, you can stop!\n",
      "\n",
      "Walkthrough:\n",
      "go east > go north > go east > go north > take coin\n",
      "\n",
      "-= Stats =-\n",
      "Nb. locations: 5\n",
      "Nb. objects: 1\n",
      "Overview image: /home/cs19s028/IIT Madras/Research/coin_collector_game_custom/coin_collector_game_tests/1.vanilla/tw_games/single_games/l5_easy_2/coin_collector_l5_easy_2.png\n"
     ]
    }
   ],
   "source": [
    "# !tw-make tw-coin_collector --level 5 -f -v --seed 1996 --save-overview --output \"tw_games/single_games/l5_easy_2/coin_collector_l5_easy_2.ulx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgyvUfnU9arT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Mapping, Any, Optional\n",
    "from collections import defaultdict, namedtuple, deque\n",
    "\n",
    "import time, datetime\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym \n",
    "import textworld.gym\n",
    "from textworld import EnvInfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aK5FTE1a9ar-"
   },
   "outputs": [],
   "source": [
    "class TextWorldGame:\n",
    "    \n",
    "    def __init__(self, game_config):\n",
    "        self.current_date_time = datetime.datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "        self.name  = f\"{game_config['name']}_\" + f\"created_on-{self.current_date_time}\"\n",
    "        self.path = game_config[\"path\"]\n",
    "        self.game_files = [self.path]\n",
    "        if os.path.isdir(self.path):\n",
    "            self.game_files = glob(os.path.join(path, \"*.ulx\"))\n",
    "            \n",
    "        self.max_steps = game_config[\"max_steps\"]\n",
    "        \n",
    "        self.env_id = textworld.gym.register_games(\n",
    "            self.game_files,\n",
    "            request_infos=self.request_infos,\n",
    "            max_episode_steps=self.max_steps\n",
    "        )\n",
    "        \n",
    "        self.env = gym.make(self.env_id)\n",
    "        \n",
    "        self.verbs = [\"go\", \"take\"]\n",
    "        self.nouns = [\"north\", \"east\", \"south\", \"west\", \"coin\"]\n",
    "        \n",
    "        self.game_info = {\n",
    "            \"env\" : self.env,\n",
    "            \"name\" : self.name,\n",
    "            \"max_steps\" : self.max_steps,\n",
    "            \"created_on\" : self.current_date_time\n",
    "        }\n",
    "        \n",
    "        self.game_controls = {\n",
    "            \"verbs\" : self.verbs,\n",
    "            \"nouns\" : self.nouns,\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        msg = f\"GAME: {self.game_info['name']}\\n\" + \\\n",
    "            \"====================================\\n\" + \\\n",
    "            f\"MAX. STEP: {self.game_info['max_step']}\\n\\n\" + \\\n",
    "            f\"VERBS: {self.game_controls['verbs']}\\n\\n\" + \\\n",
    "            f\"NOUNS: {self.game_controls['nouns']}\\n\" \n",
    "        \n",
    "        return msg\n",
    "    \n",
    "        \n",
    "    def get_game_info(self):\n",
    "        \"Return dictionary containing gym env. info.\"\n",
    "        return self.game_info\n",
    "    \n",
    "    \n",
    "    def get_game_controls(self):\n",
    "        \"Return dictionary containing gym env. info.\"\n",
    "        return self.game_controls\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def request_infos(self) -> EnvInfos:\n",
    "        request_infos = EnvInfos()\n",
    "        request_infos.description = True\n",
    "        request_infos.won = True\n",
    "        request_infos.last_command = True\n",
    "        \n",
    "        return request_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HuTywr5t9asj"
   },
   "source": [
    "## Building the random agent baseline ##\n",
    "We build an agent that plays the game by selecting random commands. This agent will be treated as the baseine agent against which the performnace of the RL agent will be measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IrtnN3YX9asn"
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \n",
    "    def __init__(self, config):  \n",
    "        \n",
    "        init_config     = config[\"init\"]\n",
    "        testing_config  = config[\"testing\"] \n",
    "        \n",
    "        self.agent_name              = init_config[\"general\"][\"agent_name\"]\n",
    "        \n",
    "        self.seed                    = init_config[\"general\"][\"seed\"]        \n",
    "        self.epochs                  = init_config[\"general\"][\"epochs\"]\n",
    "        \n",
    "        self.log_dir                 = init_config[\"general\"][\"log_dir\"]\n",
    "        self.logging_frequency       = init_config[\"general\"][\"logging_frequency\"]\n",
    "        \n",
    "        self.verbose                 = init_config[\"general\"][\"verbose\"]\n",
    "        \n",
    "        self.verbs                   = [\"go\", \"take\"]        \n",
    "        self.nouns                   = [\"north\", \"east\", \"south\", \"west\", \"coin\"]\n",
    "        self.noun_map                = dict()\n",
    "        self.verb_map                = dict()\n",
    "        \n",
    "        self._build_general_modules()\n",
    "        self._build_testing_modules(testing_config)\n",
    "        \n",
    "        \n",
    "    def _build_general_modules(self):\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "        \n",
    "        # Build verb map.\n",
    "        for i, v in enumerate(self.verbs):\n",
    "            self.verb_map[i] = v\n",
    "        # Build noun map.\n",
    "        for i, n in enumerate(self.nouns):\n",
    "            self.noun_map[i] = n\n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _build_testing_modules(self, testing_config):\n",
    "        self.max_episodes_testing = testing_config[\"scheduling\"][\"max_episodes\"]\n",
    "        self.max_steps_testing    = testing_config[\"scheduling\"][\"max_steps\"]\n",
    "            \n",
    "        return\n",
    "            \n",
    "    \n",
    "    def _choose_command(self):\n",
    "        \"\"\"Choose any command at random.\"\"\"        \n",
    "        v_idx = self.rng.randint(0, len(self.verb_map))\n",
    "        n_idx = self.rng.randint(0, len(self.noun_map))\n",
    "                \n",
    "        command_verb = self.verb_map[v_idx]\n",
    "        command_noun = self.noun_map[n_idx]\n",
    "        command      = f\"{command_verb} {command_noun}\"\n",
    "\n",
    "        return v_idx, n_idx, command\n",
    "    \n",
    "    \n",
    "    def play(self, testing_game_info, training_game_info=None, training_enabled=False):\n",
    "        \"\"\"Play the game in the selected mode.\"\"\"\n",
    "        \n",
    "        testing_game_name  = testing_game_info[\"name\"]\n",
    "        testing_game_env   = testing_game_info[\"env\"]\n",
    "        \n",
    "        # We will only return the stats corresponding to the testing phase of the agent.\n",
    "        stats = {\n",
    "            \"agent_name\"         : self.agent_name,\n",
    "            \"testing_game_name\"  : testing_game_name,\n",
    "            \"epochs\"             : self.epochs,\n",
    "            \"epochs_avg_reward\"  : [],\n",
    "            \"epochs_avg_steps\"   : [],\n",
    "            \"epochs_avg_wins\"    : []\n",
    "        }\n",
    "        \n",
    "        testing_game_name = testing_game_info[\"name\"]\n",
    "        testing_game_env  = testing_game_info[\"env\"]\n",
    "        \n",
    "        # Create log dir., if not present.\n",
    "        if not os.path.isdir(f\"{self.log_dir}_{testing_game_name}\"):\n",
    "            os.makedirs(f\"{self.log_dir}_{testing_game_name}\")\n",
    "            \n",
    "        # Create summary writer for tensorboard.\n",
    "        self.summary_writer = \\\n",
    "            tf.summary.create_file_writer(logdir=f\"{self.log_dir}_{testing_game_name}\")\n",
    "            \n",
    "        for epoch in tqdm(range(self.epochs), ascii=True, unit=\" epoch\"):\n",
    "            epoch_testing_stats = dict()\n",
    "            \n",
    "            epoch_testing_stats = self._test(testing_game_env)\n",
    "            \n",
    "            if self.verbose:\n",
    "                if (not ((epoch+1) % self.logging_frequency)) or (epoch == 0):\n",
    "                    print(\"\")\n",
    "                    print(f\"Epoch: {epoch}\")\n",
    "                    print(\"=============================================\")\n",
    "                    \n",
    "                    print(\"Test stats\")\n",
    "                    print(\"--------------------------------------------\")\n",
    "                    print(f\"Avg. reward: {epoch_testing_stats['avg_reward']:>5.3f}\")\n",
    "                    print(f\"Avg. steps: {epoch_testing_stats['avg_reward']:>5.3f}\")\n",
    "                    print(f\"Avg. wins: {epoch_testing_stats['avg_steps']:>5.3f}\")                    \n",
    "            \n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. reward\", \n",
    "                  epoch_testing_stats['avg_reward'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. steps\", \n",
    "                  epoch_testing_stats['avg_steps'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. wins\", \n",
    "                  epoch_testing_stats['avg_wins'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                \n",
    "            stats[\"epochs_avg_reward\"].append(epoch_testing_stats['avg_reward'])\n",
    "            stats[\"epochs_avg_steps\"].append(epoch_testing_stats['avg_steps'])\n",
    "            stats[\"epochs_avg_wins\"].append(epoch_testing_stats['avg_wins'])\n",
    "            \n",
    "        return stats\n",
    "    \n",
    "    \n",
    "    def _test(self, testing_game_env):\n",
    "        self.mode = \"testing\"\n",
    "        \n",
    "        testing_stats = {\n",
    "            \"avg_reward\" : 0,\n",
    "            \"avg_steps\" : 0,\n",
    "            \"avg_wins\" : 0\n",
    "        }        \n",
    "        \n",
    "        # Initialize empty list for storing episode info at the start of evry epoch.\n",
    "        episodes_rewards = []\n",
    "        episodes_steps   = []\n",
    "        episodes_won     = []\n",
    "        \n",
    "        for episode in range(self.max_episodes_testing):\n",
    "            # Reset environment at the start of evry episode.\n",
    "            obs, infos        =  testing_game_env.reset()\n",
    "            \n",
    "            cumulative_reward = 0\n",
    "            nb_moves          = 0  \n",
    "            done              = False\n",
    "            won               = False\n",
    "            \n",
    "            while not done:\n",
    "\n",
    "                v_idx, n_idx, command = self._choose_command()\n",
    "\n",
    "                obs, score, done, infos = testing_game_env.step(command)\n",
    "                \n",
    "                # Calculate reward based on score.\n",
    "                reward = score # Reward is equal to the score obtained after each move.\n",
    "\n",
    "                cumulative_reward += reward # Update cumulative reward.\n",
    "                nb_moves          += 1 # Update no. of moves (steps) played.\n",
    "                won               = infos[\"won\"]\n",
    "                \n",
    "                # Break out from while loop (i.e., end episode) if reached max. steps allowed.\n",
    "                if nb_moves >= self.max_steps_testing:\n",
    "                    break\n",
    "                    \n",
    "            episodes_rewards.append(cumulative_reward) # Rewards obtained every episode.\n",
    "            episodes_steps.append(nb_moves) # Steps played every episode.\n",
    "            episodes_won.append(won) # Info. regarding whether won episode or not.\n",
    "                \n",
    "        testing_stats[\"avg_reward\"] = np.mean(episodes_rewards)\n",
    "        testing_stats[\"avg_steps\"]  = np.mean(episodes_steps)\n",
    "        testing_stats[\"avg_wins\"]   = np.count_nonzero(episodes_won)/len(episodes_won)\n",
    "        \n",
    "        return testing_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taTFHeFC9auF"
   },
   "source": [
    "## Building the reinforcement learning agent##\n",
    "Now, we build an RL agent which uses a LSTM-DQN model to issue the commands. This agent will have to be trained first before testing it so as to measure its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3ifD0Ur9auI"
   },
   "source": [
    "### Build the LSTM-DQN model ###\n",
    "First, create the LSTM-DQN model used by the RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepresentationGenerator(tf.keras.layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super(RepresentationGenerator, self).__init__()\n",
    "        self.seed = config[\"seed\"]\n",
    "        tf.random.set_seed(self.seed)\n",
    "        self.embedding_layer_input_dim = config[\"embedding_layer_input_dim\"]\n",
    "        self.embedding_layer_hidden_dim = config[\"embedding_layer_hidden_dim\"]\n",
    "        self.lstm_layer_hidden_dim = config[\"lstm_layer_hidden_dim\"]\n",
    "        self._build_layers()\n",
    "        \n",
    "    def _build_layers(self):\n",
    "        \n",
    "        self.embedding_layer = tf.keras.layers.Embedding(\n",
    "            self.embedding_layer_input_dim,\n",
    "            self.embedding_layer_hidden_dim,\n",
    "            name=\"embedding_layer\"\n",
    "        )        \n",
    "        self.lstm_layer = tf.keras.layers.LSTM(\n",
    "            self.lstm_layer_hidden_dim, \n",
    "            return_sequences=True,\n",
    "            name=\"lstm_layer\"\n",
    "        )        \n",
    "        self.mean_pooling_layer = tf.keras.layers.AveragePooling1D(\n",
    "            name=\"mean_pooling_layer\"\n",
    "        )\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding_layer(x)        \n",
    "        x = self.lstm_layer(x)        \n",
    "        x = self.mean_pooling_layer(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionScorer(tf.keras.layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super(ActionScorer, self).__init__()\n",
    "        self.seed = config[\"seed\"]\n",
    "        tf.random.set_seed(self.seed)\n",
    "        self.shared_mlp_layer_hidden_dim = config[\"shared_mlp_layer_hidden_dim\"]\n",
    "        self.verb_scorer_perceptron_hidden_dim = config[\"verb_scorer_perceptron_hidden_dim\"]\n",
    "        self.noun_scorer_perceptron_hidden_dim = config[\"noun_scorer_perceptron_hidden_dim\"]\n",
    "        self.verb_scorer_output_dim = config[\"verb_scorer_output_dim\"]\n",
    "        self.noun_scorer_output_dim = config[\"noun_scorer_output_dim\"]\n",
    "        self._build_layers()\n",
    "        \n",
    "    def _build_layers(self):\n",
    "        self.shared_mlp_layer = tf.keras.layers.Dense(\n",
    "            self.shared_mlp_layer_hidden_dim, \n",
    "            activation=\"relu\",\n",
    "            name=\"shared_mlp_layer\"\n",
    "        )        \n",
    "        self.verb_scorer_perceptron_layer = tf.keras.layers.Dense(\n",
    "            self.verb_scorer_perceptron_hidden_dim,\n",
    "            activation=\"relu\",\n",
    "            name=\"verb_scorer_perceptron_layer\"\n",
    "        )        \n",
    "        self.noun_scorer_perceptron_layer = tf.keras.layers.Dense(\n",
    "            self.noun_scorer_perceptron_hidden_dim,\n",
    "            activation=\"relu\",\n",
    "            name=\"noun_scorer_perceptron_layer\"\n",
    "        )\n",
    "        self.verb_scorer_layer = tf.keras.layers.Dense(\n",
    "            self.verb_scorer_output_dim, \n",
    "            name=\"verb_scores\"\n",
    "        )        \n",
    "        self.noun_scorer_layer = tf.keras.layers.Dense(\n",
    "            self.noun_scorer_output_dim, \n",
    "            name=\"noun_scores\"\n",
    "        )\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.shared_mlp_layer(x)\n",
    "        \n",
    "        x1 = self.verb_scorer_perceptron_layer(x)\n",
    "        x1 = self.verb_scorer_layer(x1)\n",
    "        \n",
    "        x2 = self.noun_scorer_perceptron_layer(x)\n",
    "        x2 = self.noun_scorer_layer(x2)\n",
    "        \n",
    "        return [x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_DQN(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super(LSTM_DQN, self).__init__()\n",
    "        \n",
    "        self.representation_generator_config = {\n",
    "            \"seed\"                       : config[\"general\"][\"seed\"],\n",
    "            \"embedding_layer_input_dim\" : config[\"layers\"][\"embedding_input_dim\"],\n",
    "            \"embedding_layer_hidden_dim\" : config[\"layers\"][\"embedding_hidden_dim\"],\n",
    "            \"lstm_layer_hidden_dim\"      : config[\"layers\"][\"lstm_hidden_dim\"]\n",
    "        }\n",
    "        \n",
    "        self.action_scorer_config = {\n",
    "            \"seed\"                              : config[\"general\"][\"seed\"],\n",
    "            \"shared_mlp_layer_hidden_dim\"       : config[\"layers\"][\"shared_mlp_hidden_dim\"],\n",
    "            \"verb_scorer_perceptron_hidden_dim\" : config[\"layers\"][\"verb_scorer_perceptron_hidden_dim\"],\n",
    "            \"noun_scorer_perceptron_hidden_dim\" : config[\"layers\"][\"noun_scorer_perceptron_hidden_dim\"],\n",
    "            \"verb_scorer_output_dim\"            : config[\"layers\"][\"verb_scorer_output_dim\"],\n",
    "            \"noun_scorer_output_dim\"            : config[\"layers\"][\"noun_scorer_output_dim\"]        \n",
    "        }\n",
    "        \n",
    "        self._build_layers()\n",
    "        \n",
    "        \n",
    "    def _build_layers(self):\n",
    "        self.representation_generator = RepresentationGenerator(self.representation_generator_config)\n",
    "        self.action_scorer = ActionScorer(self.action_scorer_config)\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.representation_generator(x)\n",
    "        x1, x2 = self.action_scorer(x)\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uv7zUsDS9auY"
   },
   "source": [
    "### Transition Tuple ###\n",
    "Create a _nametuple_ called *Transitions* to store the experiences in the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSo_v9Us9aub"
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    \"Transition\",\n",
    "    (\"state\", \"v_idx\", \"n_idx\", \"reward\",\n",
    "     \"discovery_bonus\", \"next_state\", \"done\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYq0y_1o9aul"
   },
   "source": [
    "### Replay Memory###\n",
    "This is a vanilla replay memory used to store the experiences such that the stored experiences can be used later on for training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kk-x_YZj9aun"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, seed, capacity=100_000):\n",
    "        # vanilla replay memory\n",
    "        self.seed = seed\n",
    "        random.seed(self.seed)\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, minibatch_size):\n",
    "        return random.sample(self.memory, minibatch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owkf7QM19au1"
   },
   "source": [
    "### Prioritized Replay Memory Class ###\n",
    "We now create a prioritized replay memory to store the experiences so that they can be used later on for training the agent. Using the vanilla replay memory proved to be little disadvanatgeous due to the problem of **catatrophic forgetting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mQ8GGrm9au4"
   },
   "outputs": [],
   "source": [
    "class PrioritizedReplayMemory(object):\n",
    "\n",
    "    def __init__(self, seed, capacity=100_000, priority_fraction=0.0):\n",
    "        # prioritized replay memory\n",
    "        self.seed = seed\n",
    "        random.seed(self.seed)\n",
    "        self.priority_fraction = priority_fraction\n",
    "        self.alpha_capacity = int(capacity * priority_fraction)\n",
    "        self.beta_capacity = capacity - self.alpha_capacity\n",
    "        self.alpha_memory = []\n",
    "        self.beta_memory = []\n",
    "        self.alpha_position = 0 \n",
    "        self.beta_position = 0\n",
    "\n",
    "    def push(self, is_prior=False, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if is_prior:\n",
    "            if len(self.alpha_memory) < self.alpha_capacity:\n",
    "                self.alpha_memory.append(None)\n",
    "            self.alpha_memory[self.alpha_position] = Transition(*args)\n",
    "            self.alpha_position = (self.alpha_position + 1) % self.alpha_capacity\n",
    "        else:\n",
    "            if len(self.beta_memory) < self.beta_capacity:\n",
    "                self.beta_memory.append(None)\n",
    "            self.beta_memory[self.beta_position] = Transition(*args)\n",
    "            self.beta_position = (self.beta_position + 1) % self.beta_capacity\n",
    "\n",
    "    def sample(self, minibatch_size):\n",
    "        from_alpha = min(int(self.priority_fraction * minibatch_size), len(self.alpha_memory))\n",
    "        from_beta = min(minibatch_size - int(self.priority_fraction * minibatch_size), len(self.beta_memory))\n",
    "        res = random.sample(self.alpha_memory, from_alpha) + random.sample(self.beta_memory, from_beta)\n",
    "        random.shuffle(res)\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alpha_memory) + len(self.beta_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fzg_mleM9avE"
   },
   "source": [
    "### Reinforcement learning agent ###\n",
    "Now, we finally create the reinforcement learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LgIQHia9avH"
   },
   "outputs": [],
   "source": [
    "class RLAgent: \n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        init_config     = config[\"init\"]\n",
    "        training_config = config[\"training\"]\n",
    "        testing_config  = config[\"testing\"] \n",
    "        \n",
    "        self.agent_name              = init_config[\"general\"][\"agent_name\"]\n",
    "        \n",
    "        self.seed                    = init_config[\"general\"][\"seed\"]\n",
    "        self.max_vocab_size          = init_config[\"general\"][\"max_vocab_size\"]\n",
    "        self.epochs                  = init_config[\"general\"][\"epochs\"]\n",
    "        self.log_dir                 = init_config[\"general\"][\"log_dir\"]\n",
    "        self.logging_frequency       = init_config[\"general\"][\"logging_frequency\"]\n",
    "        \n",
    "        self.model_dir               = init_config[\"general\"][\"model_dir\"]\n",
    "        self.model_saving_frequency  = init_config[\"general\"][\"model_saving_frequency\"]\n",
    "        \n",
    "        self.verbose                 = init_config[\"general\"][\"verbose\"]\n",
    "        self.model_config            = init_config[\"model\"][\"lstm_dqn\"]\n",
    "        self.learning_rate           = self.model_config[\"optimizer\"][\"learning_rate\"]\n",
    "        \n",
    "        self.id2word                 = [\"<PAD>\", \"<UNK>\"]\n",
    "        self.word2id                 = {w: i for i, w in enumerate(self.id2word)}\n",
    "        \n",
    "        self.verbs                   = [\"go\", \"take\"]        \n",
    "        self.nouns                   = [\"north\", \"east\", \"south\", \"west\", \"coin\"]\n",
    "        self.noun_map                = dict()\n",
    "        self.verb_map                = dict()\n",
    "        \n",
    "        self._build_general_modules()\n",
    "        self._build_training_modules(training_config)\n",
    "        self._build_testing_modules(testing_config)\n",
    "        \n",
    "    \n",
    "    def _build_general_modules(self): \n",
    "        \n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "        \n",
    "        # Build verb map.\n",
    "        for i, v in enumerate(self.verbs):\n",
    "            self.verb_map[i] = v\n",
    "            \n",
    "        # Build noun map.\n",
    "        for i, n in enumerate(self.nouns):\n",
    "            self.noun_map[i] = n\n",
    "        \n",
    "        # Add the information abot number of verbs and nouns to\n",
    "        # model config dicitionary.\n",
    "        self.model_config[\"layers\"][\"verb_scorer_output_dim\"] = len(self.verb_map)\n",
    "        self.model_config[\"layers\"][\"noun_scorer_output_dim\"] = len(self.noun_map)        \n",
    "        \n",
    "        # Create new main lstm-dqn model (if loading saved model is set to false).\n",
    "        if self.model_config[\"general\"][\"load_saved_model\"] == True:\n",
    "            self.main_model = tf.keras.models.load_model(self.model_config[\"general\"][\"saved_model_path\"])\n",
    "        else:\n",
    "            self.main_model = self._create_model(self.model_config)\n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def _create_model(self, model_name):\n",
    "        \"\"\"Method to create the LSTM-DQN model.\"\"\"\n",
    "        # Create the model.\n",
    "        lstm_dqn = LSTM_DQN(self.model_config)\n",
    "        \n",
    "        # Compile the model.\n",
    "        lstm_dqn.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(self.learning_rate),\n",
    "            loss=tf.keras.losses.MSE,\n",
    "            metrics=\"accuracy\"\n",
    "        )\n",
    "    \n",
    "        return lstm_dqn\n",
    "    \n",
    "    \n",
    "    def _build_training_modules(self, training_config):        \n",
    "        self.max_episodes_training    = training_config[\"scheduling\"][\"max_episodes\"]\n",
    "        self.max_steps_training       = training_config[\"scheduling\"][\"max_steps\"]\n",
    "        self.bonus_coefficient        = 0 # No bonus coefficient for vanilla agents.\n",
    "        self.discount                 = training_config[\"scheduling\"][\"discount\"]\n",
    "        \n",
    "        self.replay_frequency         = training_config[\"scheduling\"][\"replay_frequency\"]\n",
    "        self.replay_counter           = 0\n",
    "        self.minibatch_size           = training_config[\"scheduling\"][\"minibatch_size\"]\n",
    "        self.target_update_frequency  = training_config[\"scheduling\"][\"target_update_frequency\"]\n",
    "        self.target_update_counter    = 0\n",
    "        self.min_replay_memory_size   = training_config[\"scheduling\"][\"min_replay_memory_size\"]\n",
    "        \n",
    "        self.replay_memory_capacity   = training_config[\"replay_memory\"][\"capacity\"]\n",
    "        self.replay_priority_fraction = training_config[\"replay_memory\"][\"priority_fraction\"]\n",
    "        \n",
    "        self.epsilon_init_training    = training_config[\"exploration\"][\"epsilon_init\"]\n",
    "        self.epsilon_min_training     = training_config[\"exploration\"][\"epsilon_min\"]\n",
    "        self.epsilon_decay_training   = training_config[\"exploration\"][\"epsilon_decay_rate\"]\n",
    "        self.epsilon_training         = self.epsilon_init_training\n",
    "        \n",
    "        # Target neural n/w model.\n",
    "        self.target_model = self._create_model(self.model_config)\n",
    "        self.target_model.set_weights(self.main_model.get_weights())\n",
    "        \n",
    "        # Replay memory.\n",
    "        self.replay_memory = PrioritizedReplayMemory(\n",
    "            seed=self.seed, \n",
    "            capacity=self.replay_memory_capacity, \n",
    "            priority_fraction=self.replay_priority_fraction\n",
    "        )\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _build_testing_modules(self, testing_config):\n",
    "        self.max_episodes_testing = testing_config[\"scheduling\"][\"max_episodes\"]\n",
    "        self.max_steps_testing    = testing_config[\"scheduling\"][\"max_steps\"]\n",
    "        self.epsilon_testing      = testing_config[\"exploration\"][\"epsilon_init\"]\n",
    "            \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def _choose_command(self, state, eps):\n",
    "        \"\"\"Choose command using e-greedy strategy.\"\"\"\n",
    "        if np.random.random() >= eps:\n",
    "            # Exploit.\n",
    "            v_idx, n_idx = self._get_max_qs_idx(self.main_model, [state])\n",
    "            v_idx = v_idx[0]\n",
    "            n_idx = n_idx[0]\n",
    "        else:\n",
    "            # Explore.\n",
    "            v_idx = self.rng.randint(0, len(self.verb_map))\n",
    "            n_idx = self.rng.randint(0, len(self.noun_map))\n",
    "                \n",
    "        command_verb = self.verb_map[v_idx]\n",
    "        command_noun = self.noun_map[n_idx]\n",
    "        command      = f\"{command_verb} {command_noun}\"\n",
    "\n",
    "        return v_idx, n_idx, command\n",
    "    \n",
    "    \n",
    "    def play(self, testing_game_info, training_game_info=None, training_enabled=False):\n",
    "        \"\"\"Play the game in the selected mode.\"\"\"\n",
    "        \n",
    "        if training_enabled == True:\n",
    "            training_game_name = training_game_info[\"name\"]\n",
    "            training_game_env  = training_game_info[\"env\"]\n",
    "        \n",
    "        testing_game_name  = testing_game_info[\"name\"]\n",
    "        testing_game_env   = testing_game_info[\"env\"]\n",
    "        \n",
    "         # Create log dir., if not present.\n",
    "        if not os.path.isdir(f\"{self.log_dir}_{testing_game_name}\"):\n",
    "            os.makedirs(f\"{self.log_dir}_{testing_game_name}\")\n",
    "            \n",
    "        # Create summary writer for tensorboard.\n",
    "        self.summary_writer = \\\n",
    "            tf.summary.create_file_writer(logdir=f\"{self.log_dir}_{testing_game_name}\")\n",
    "        \n",
    "        # Create model dir., if not present.\n",
    "        if not os.path.isdir(f\"{self.model_dir}_{testing_game_name}\"):\n",
    "            os.makedirs(f\"{self.model_dir}_{testing_game_name}\")\n",
    "        \n",
    "        # We will only return the stats c,orresponding to the testing phase of the agent.\n",
    "        stats = {\n",
    "            \"agent_name\"         : self.agent_name,\n",
    "            \"testing_game_name\"  : testing_game_name,\n",
    "            \"epochs\"             : self.epochs,\n",
    "            \"epochs_avg_reward\"  : [],\n",
    "            \"epochs_avg_steps\"   : [],\n",
    "            \"epochs_avg_wins\"    : []\n",
    "        }\n",
    "        \n",
    "        # Create log dir if not present.\n",
    "        if not os.path.isdir(f\"{self.log_dir}_{testing_game_name}\"):\n",
    "            os.makedirs(f\"{self.log_dir}_{testing_game_name}\")\n",
    "        \n",
    "        for epoch in tqdm(range(self.epochs), ascii=True, unit=\" epoch\"):\n",
    "            epoch_training_stats = dict()\n",
    "            epoch_testing_stats = dict()\n",
    "            \n",
    "            if training_enabled:\n",
    "                epoch_training_stats = self._train(training_game_env)\n",
    "\n",
    "            epoch_testing_stats = self._test(testing_game_env)\n",
    "            \n",
    "            if self.verbose:\n",
    "                if (not ((epoch+1) % self.logging_frequency)) or (epoch == 0):\n",
    "                    print(\"\")\n",
    "                    print(f\"Epoch: {epoch}\")\n",
    "                    print(\"=============================================\")\n",
    "                    \n",
    "                    if training_enabled:\n",
    "                        print(\"Train stats\")\n",
    "                        print(\"--------------------------------------------\")\n",
    "                        print(f\"Avg. reward: {epoch_training_stats['avg_reward']:>5.3f}\")\n",
    "                        print(f\"Avg. steps: {epoch_training_stats['avg_steps']:>5.3f}\")\n",
    "                        print(f\"Avg. wins: {epoch_training_stats['avg_wins']:>5.3f}\")\n",
    "                        print(f\"Epsilon: {self.epsilon_training:>5.3f}\")\n",
    "                    \n",
    "                    print(\"\")\n",
    "                    print(\"Test stats\")\n",
    "                    print(\"--------------------------------------------\")\n",
    "                    print(f\"Avg. reward: {epoch_testing_stats['avg_reward']:>5.3f}\")\n",
    "                    print(f\"Avg. steps: {epoch_testing_stats['avg_steps']:>5.3f}\")\n",
    "                    print(f\"Avg. wins: {epoch_testing_stats['avg_wins']:>5.3f}\")\n",
    "                    print(f\"Epsilon: {self.epsilon_testing:>5.3f}\")\n",
    "                    \n",
    "            \n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. reward\", \n",
    "                  epoch_testing_stats['avg_reward'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. steps\", \n",
    "                  epoch_testing_stats['avg_steps'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                  f\"Avg. wins\", \n",
    "                  epoch_testing_stats['avg_wins'],\n",
    "                  step=epoch\n",
    "                )\n",
    "                \n",
    "            if (not ((epoch+1) % self.model_saving_frequency)):\n",
    "                model_name = \\\n",
    "                    f\"epoch-{epoch}_\" + f\"avg_reward-{epoch_testing_stats['avg_reward']}_\" + \\\n",
    "                    f\"avg_steps-{epoch_testing_stats['avg_steps']}_\" + \\\n",
    "                    f\"avg_wins-{epoch_testing_stats['avg_wins']}_\" + \\\n",
    "                    f\"training_epsilon-{self.epsilon_training}\"\n",
    "                model_out = os.path.join(f\"{self.model_dir}_{testing_game_name}_mode-{self.mode}\", model_name)\n",
    "                self.main_model.save(model_out)\n",
    "                \n",
    "            stats[\"epochs_avg_reward\"].append(epoch_testing_stats['avg_reward'])\n",
    "            stats[\"epochs_avg_steps\"].append(epoch_testing_stats['avg_steps'])\n",
    "            stats[\"epochs_avg_wins\"].append(epoch_testing_stats['avg_wins'])\n",
    "            \n",
    "        return stats\n",
    "    \n",
    "    \n",
    "    def _train(self, training_game_env):\n",
    "        self.mode = \"training\"\n",
    "        \n",
    "        training_stats = {\n",
    "            \"avg_reward\" : 0,\n",
    "            \"avg_steps\" : 0,\n",
    "            \"avg_wins\" : 0\n",
    "        }        \n",
    "        \n",
    "        # Initialize empty list for storing episode info at the start of evry epoch.\n",
    "        episodes_rewards = []\n",
    "        episodes_steps   = []\n",
    "        episodes_won     = [] \n",
    "        \n",
    "        for episode in range(self.max_episodes_training):\n",
    "        \n",
    "            # Reset environment at the start of evry episode.\n",
    "            obs, infos        = training_game_env.reset()\n",
    "\n",
    "            state             = f\"{obs}\\n{infos['description']}\\n{infos['last_command']}\"            \n",
    "            state_processed   = self._preprocess([state])\n",
    "            cumulative_reward = 0\n",
    "            nb_moves          = 0  \n",
    "            done              = False\n",
    "            won               = False\n",
    "            \n",
    "            while not done:\n",
    "\n",
    "                v_idx, n_idx, command = self._choose_command(state_processed, self.epsilon_training)\n",
    "\n",
    "                obs, score, done, infos = training_game_env.step(command)\n",
    "\n",
    "                # Calculate reward based on score.\n",
    "                reward = score # Reward is equal to the score obtained after each move. \n",
    "                \n",
    "                # No discovery bonus for vanilla agent.    \n",
    "                discovery_bonus = 0\n",
    "\n",
    "                # Preprocess the next state.\n",
    "                next_state = f\"{obs}\\n{infos['description']}\\n{infos['last_command']}\"\n",
    "                next_state_processed = self._preprocess([next_state])                \n",
    "\n",
    "                # In training mode, we need to remember experiences and replay every few steps.\n",
    "                if reward >= 0.0:\n",
    "                    is_prior = True\n",
    "                else:\n",
    "                    is_prior = False\n",
    "                    \n",
    "                self._remember(\n",
    "                    is_prior, state_processed, v_idx, n_idx, reward, \n",
    "                    discovery_bonus, next_state_processed, done\n",
    "                )\n",
    "\n",
    "                #  After evry move, increment replay_counter.\n",
    "                self.replay_counter += 1\n",
    "                if self.replay_counter > self.replay_frequency: \n",
    "                    self._replay()\n",
    "                    self.replay_counter = 0\n",
    "                    \n",
    "                \n",
    "\n",
    "                state             = next_state\n",
    "                state_processed   = next_state_processed # Update current state.\n",
    "                cumulative_reward += reward # Update cumulative reward.\n",
    "                nb_moves          += 1 # Update no. of moves (steps) played.\n",
    "                won               = infos[\"won\"]\n",
    "                \n",
    "                self.epsilon_training = max(self.epsilon_min_training, self.epsilon_decay_training * self.epsilon_training)\n",
    "                \n",
    "                # Break out from while loop (i.e., end episode) if reached max. steps allowed.\n",
    "                if nb_moves >= self.max_steps_training:\n",
    "                    break\n",
    "                    \n",
    "            episodes_rewards.append(cumulative_reward) # Rewards obtained every episode.\n",
    "            episodes_steps.append(nb_moves) # Steps played every episode.\n",
    "            episodes_won.append(won) # Info. regarding whether won episode or not.\n",
    "\n",
    "            if self.mode == \"training\":\n",
    "                # If in training mode, increment target_update_counter at end of every episode.\n",
    "                self.target_update_counter += 1\n",
    "                if self.target_update_counter > self.target_update_frequency:\n",
    "                    self.target_model.set_weights(self.main_model.get_weights())\n",
    "                    self.target_update_counter = 0\n",
    "                \n",
    "        training_stats[\"avg_reward\"] = np.mean(episodes_rewards)\n",
    "        training_stats[\"avg_steps\"]  = np.mean(episodes_steps)\n",
    "        training_stats[\"avg_wins\"]   = np.count_nonzero(episodes_won)/len(episodes_won)\n",
    "        \n",
    "        return training_stats\n",
    "    \n",
    "    \n",
    "    def _test(self, testing_game_env):\n",
    "        self.mode = \"testing\"\n",
    "        \n",
    "        testing_stats = {\n",
    "            \"avg_reward\" : 0,\n",
    "            \"avg_steps\" : 0,\n",
    "            \"avg_wins\" : 0\n",
    "        }        \n",
    "        \n",
    "        # Initialize empty list for storing episode info at the start of evry epoch.\n",
    "        episodes_rewards = []\n",
    "        episodes_steps   = []\n",
    "        episodes_won     = []\n",
    "        \n",
    "        for episode in range(self.max_episodes_testing):\n",
    "            # Reset environment at the start of evry episode.\n",
    "            obs, infos        =  testing_game_env.reset()\n",
    "\n",
    "            state             = f\"{obs}\\n{infos['description']}\\n{infos['last_command']}\"\n",
    "            state_processed   = self._preprocess([state])\n",
    "            cumulative_reward = 0\n",
    "            nb_moves          = 0  \n",
    "            done              = False\n",
    "            won               = False\n",
    "            \n",
    "            while not done:\n",
    "\n",
    "                v_idx, n_idx, command = self._choose_command(state_processed, self.epsilon_testing)\n",
    "\n",
    "                obs, score, done, infos = testing_game_env.step(command)\n",
    "\n",
    "                # Calculate reward based on score.\n",
    "                reward = score # Reward is equal to the score obtained after each move.               \n",
    "\n",
    "                # Preprocess the next state.\n",
    "                next_state = f\"{obs}\\n{infos['description']}\\n{infos['last_command']}\"\n",
    "                next_state_processed = self._preprocess([next_state])\n",
    "\n",
    "                state             = next_state\n",
    "                state_processed   = next_state_processed # Update current state.\n",
    "                cumulative_reward += reward # Update cumulative reward.\n",
    "                nb_moves          += 1 # Update no. of moves (steps) played.\n",
    "                won               = infos[\"won\"]\n",
    "                \n",
    "                # Break out from while loop (i.e., end episode) if reached max. steps allowed.\n",
    "                if nb_moves >= self.max_steps_testing:\n",
    "                    break\n",
    "                    \n",
    "            episodes_rewards.append(cumulative_reward) # Rewards obtained every episode.\n",
    "            episodes_steps.append(nb_moves) # Steps played every episode.\n",
    "            episodes_won.append(won) # Info. regarding whether won episode or not.\n",
    "                \n",
    "        testing_stats[\"avg_reward\"] = np.mean(episodes_rewards)\n",
    "        testing_stats[\"avg_steps\"]  = np.mean(episodes_steps)\n",
    "        testing_stats[\"avg_wins\"]   = np.count_nonzero(episodes_won)/len(episodes_won)\n",
    "        \n",
    "        return testing_stats\n",
    "    \n",
    "    \n",
    "    def _remember(self, is_prior, state, v_idx, n_idx, reward, discovery_bonus, next_state, done):\n",
    "        \"\"\"Update replay memory.\"\"\"\n",
    "        self.replay_memory.push(is_prior, state, v_idx, n_idx, reward, discovery_bonus, next_state, done)\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def _get_max_qs_idx(self, nn_model, state_batch):\n",
    "        \"\"\"Get indexes corresponding to the max. verb and noun scores.\"\"\"\n",
    "        \n",
    "        # Add padding to the batch.\n",
    "        state_batch = self._add_padding(state_batch)\n",
    "        \n",
    "        verb_scores_batch, noun_scores_batch = \\\n",
    "            nn_model.predict(state_batch)\n",
    "        \n",
    "        max_verb_score_idx_batch = np.argmax(verb_scores_batch, axis=2)\n",
    "        max_verb_score_idx_batch = max_verb_score_idx_batch[:, 0]\n",
    "        \n",
    "        max_noun_score_idx_batch = np.argmax(noun_scores_batch, axis=2)\n",
    "        max_noun_score_idx_batch = max_noun_score_idx_batch[:, 0]\n",
    "        \n",
    "        return max_verb_score_idx_batch, max_noun_score_idx_batch\n",
    "    \n",
    "    \n",
    "    def _get_max_qs(self, nn_model, state_batch):\n",
    "        \"\"\"Get max. verb and noun scores.\"\"\"\n",
    "        \n",
    "        # Add padding to the batch.\n",
    "        state_batch = self._add_padding(state_batch)\n",
    "        \n",
    "        verb_scores_batch, noun_scores_batch = \\\n",
    "            nn_model.predict(state_batch)\n",
    "        \n",
    "        max_verb_score_batch = np.max(verb_scores_batch, axis=2)\n",
    "\n",
    "        max_verb_score_batch = max_verb_score_batch[:, 0]\n",
    "        \n",
    "        max_noun_score_batch = np.max(noun_scores_batch, axis=2)\n",
    "        max_noun_score_batch = max_noun_score_batch[:, 0]\n",
    "        \n",
    "        return max_verb_score_batch, max_noun_score_batch\n",
    "    \n",
    "    \n",
    "    def _get_qs(self, nn_model, state_batch):\n",
    "        \"\"\"Get list of verb scores and list of noun scores.\"\"\"\n",
    "        \n",
    "        # Add padding to the batch.\n",
    "        state_batch = self._add_padding(state_batch)\n",
    "        \n",
    "        verb_scores_batch, noun_scores_batch = \\\n",
    "            nn_model.predict(state_batch)\n",
    "        \n",
    "        return verb_scores_batch, noun_scores_batch\n",
    "    \n",
    "    \n",
    "    def _get_target_scores(self, idx_batch, scores_batch, target_batch):\n",
    "        \"\"\"\n",
    "        Create the batch of target scores (verb/noun) \n",
    "        used for fitting the model.\n",
    "        \"\"\"\n",
    "        scores_target_batch = []\n",
    "        \n",
    "        for idx, scores, target in zip(idx_batch, scores_batch, target_batch):\n",
    "            scores_target = np.copy(scores)\n",
    "            scores_target[0, idx] = target\n",
    "            scores_target_batch.append(scores_target)\n",
    "        \n",
    "        scores_target_batch = tf.constant(scores_target_batch)\n",
    "        \n",
    "        return scores_target_batch\n",
    "    \n",
    "    \n",
    "    def _replay(self):\n",
    "        \"\"\"Creat source-target pairs and perform gradient update. \"\"\"\n",
    "        \n",
    "        # Sample a minibatch of transitions from the \n",
    "        # experience replay memory and create req. lists\n",
    "        # to perform nececessary operations on.\n",
    "        \n",
    "        if len(self.replay_memory) < self.min_replay_memory_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = self.replay_memory.sample(self.minibatch_size)\n",
    "        \n",
    "        state_batch           = [data.state for data in minibatch]        \n",
    "        v_idx_batch           = [data.v_idx for data in minibatch]        \n",
    "        n_idx_batch           = [data.n_idx for data in minibatch]        \n",
    "        reward_batch          = [data.reward for data in minibatch]\n",
    "        discovery_bonus_batch = [data.discovery_bonus for data in minibatch]\n",
    "        next_state_batch      = [data.next_state for data in minibatch]        \n",
    "        done_batch            = [data.done for data in minibatch]\n",
    "        \n",
    "        # Get the batch of max. verb and noun scores corresponding\n",
    "        # to the next states.\n",
    "        max_verb_score_batch, max_noun_score_batch = \\\n",
    "            self._get_max_qs(self.target_model, next_state_batch)\n",
    "        \n",
    "        # Get the Q-learning targets for verb scores.\n",
    "        verb_target_batch = []\n",
    "        for done, reward, discovery_bonus, max_verb_score in \\\n",
    "        zip(done_batch, reward_batch, discovery_bonus_batch, max_verb_score_batch):\n",
    "            if done:\n",
    "                verb_target = (reward + discovery_bonus)\n",
    "            else:\n",
    "                verb_target = (reward + discovery_bonus) + (self.discount * max_verb_score)\n",
    "            \n",
    "            verb_target_batch.append(verb_target)\n",
    "        \n",
    "        # Get the Q-learning targets for noun scores.\n",
    "        noun_target_batch = []\n",
    "        for done, reward, discovery_bonus, max_noun_score in \\\n",
    "        zip(done_batch, reward_batch, discovery_bonus_batch, max_noun_score_batch):\n",
    "            if done:\n",
    "                noun_target = (reward + discovery_bonus)\n",
    "            else:\n",
    "                noun_target = (reward + discovery_bonus) + (self.discount * max_noun_score)\n",
    "            \n",
    "            noun_target_batch.append(noun_target)\n",
    "        \n",
    "        # Get the verb and noun scores corresponding to batch of \n",
    "        # current state.\n",
    "        verb_scores_batch, noun_scores_batch = \\\n",
    "            self._get_qs(self.main_model, state_batch)   \n",
    "        \n",
    "        # IMP: Create the batch of target verb scores.\n",
    "        verb_scores_target_batch = self._get_target_scores(\n",
    "            v_idx_batch, verb_scores_batch,\n",
    "            verb_target_batch\n",
    "        )\n",
    "        \n",
    "        # IMP: Create the batch of target verb scores.\n",
    "        noun_scores_target_batch = self._get_target_scores(\n",
    "            n_idx_batch, noun_scores_batch,\n",
    "            noun_target_batch\n",
    "        )        \n",
    "            \n",
    "        # IMP: Create the batch of source input sequence.\n",
    "        input_seq_data_batch = self._add_padding(state_batch)        \n",
    "\n",
    "        # Fit the main model.\n",
    "        self.main_model.fit(\n",
    "            input_seq_data_batch,\n",
    "            [verb_scores_target_batch,noun_scores_target_batch],\n",
    "            batch_size=self.minibatch_size,\n",
    "            verbose=False,\n",
    "            shuffle=False\n",
    "        )\n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _get_word_id(self, word):\n",
    "        if word not in self.word2id:\n",
    "            if len(self.word2id) >= self.max_vocab_size:\n",
    "                return self.word2id[\"<UNK>\"]\n",
    "            \n",
    "            self.id2word.append(word)\n",
    "            self.word2id[word] = len(self.word2id)\n",
    "            \n",
    "        return self.word2id[word]\n",
    "    \n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
    "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
    "        word_ids = list(map(self._get_word_id, text.split()))\n",
    "        return word_ids\n",
    "\n",
    "    \n",
    "    def _add_padding(self, texts):\n",
    "        \"\"\"Add padding to the batch of texts.\"\"\"\n",
    "        max_len = max(len(text[0]) for text in texts)\n",
    "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            padded[i, :len(text[0])] = text\n",
    "            \n",
    "        padded_texts = np.copy(padded)\n",
    "        return padded_texts\n",
    "    \n",
    "    \n",
    "    def _preprocess(self, texts):\n",
    "        \"\"\"Tokenize and padding (if req.) the text.\"\"\"\n",
    "        texts = list(map(self._tokenize, texts))\n",
    "        texts = np.array(texts)\n",
    "        \n",
    "        return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eF966QQ_9atI"
   },
   "source": [
    "## Method to evaluate performance of agent\n",
    "Next , we define a method to evaluate performance of a (**random/RL**) agent as it plays any (**text based**) game in (**train/test**) mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCwZuAHD9atO"
   },
   "outputs": [],
   "source": [
    "def evaluate_performance(agent, training_game_config, testing_game_config, training_enabled=False):\n",
    "    \n",
    "    # Create textworld object anf get req. info to play game.\n",
    "    text_world_game_testing = TextWorldGame(testing_game_config)\n",
    "    testing_game_info = text_world_game_testing.get_game_info()\n",
    "    \n",
    "    if training_enabled:\n",
    "        text_world_game_training = TextWorldGame(training_game_config)\n",
    "        training_game_info = text_world_game_training.get_game_info()\n",
    "    else:\n",
    "        training_game_info = None\n",
    "    \n",
    "    stats = agent.play(testing_game_info, training_game_info, training_enabled)\n",
    "        \n",
    "    # Stats containing info. related to the diff. epochs.    \n",
    "    agent_name        = stats[\"agent_name\"]\n",
    "    testing_game_name = stats[\"testing_game_name\"]\n",
    "    epochs            = stats[\"epochs\"]\n",
    "    epochs_avg_reward = stats[\"epochs_avg_reward\"]\n",
    "    epochs_avg_steps  = stats[\"epochs_avg_steps\"]\n",
    "    epochs_avg_wins   = stats[\"epochs_avg_wins\"]\n",
    "    \n",
    "    plt_path = os.path.join(\"plots\", f\"{agent_name}_{testing_game_name}\")\n",
    "    if not os.path.isdir(plt_path):\n",
    "        os.makedirs(plt_path)\n",
    "    \n",
    "    \"\"\"Plot the learning curves.\"\"\"\n",
    "    \n",
    "    # Fig 1: Avg. reward over all epochs.\n",
    "    fig = plt.figure(figsize=(10, 6))    \n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_title(\n",
    "        f\"GAME- {testing_game_name}\\n\" + \\\n",
    "        f\"========================================================\\n\" + \\\n",
    "        f\"Avg. rewards over {epochs} epochs\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Avg. reward\")\n",
    "    ax.plot(range(epochs), epochs_avg_reward)\n",
    "    plt.savefig(f\"{plt_path}/avg_rewards.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Fig 2: Avg. steps over all epochs.\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_title(\n",
    "        f\"GAME- {testing_game_name}\\n\" + \\\n",
    "        f\"========================================================\\n\" + \\\n",
    "        f\"Avg. steps over {epochs} epochs\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Avg. steps\")\n",
    "    ax.plot(range(epochs), epochs_avg_steps)\n",
    "    plt.savefig(f\"{plt_path}/avg_steps.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Fig 3: Avg. steps over all epochs.\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_title(\n",
    "        f\"GAME- {testing_game_name}\\n\" + \\\n",
    "        f\"========================================================\\n\" + \\\n",
    "        f\"Avg. wins over {epochs} epochs\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Avg. wins\")\n",
    "    ax.plot(range(epochs), epochs_avg_wins)\n",
    "    plt.savefig(f\"{plt_path}/avg_wins.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0peEwESP9ati"
   },
   "source": [
    "## Get games for evaluating agent performances##\n",
    "Here, we obtain the trainig games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_config = dict()\n",
    "game_config[\"name\"] = \"coin_collector_l5_easy_2\"\n",
    "game_config[\"max_steps\"] = 50\n",
    "game_config[\"path\"] = \"tw_games/single_games/l5_easy_2/coin_collector_l5_easy_2.ulx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get that agents to play the game ##\n",
    "Now, we get the **Random Agent** and **RL Agent** to play the coin collectr game having 5 rooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random agent ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read random agent config details from config file.\n",
    "config_file = open(\"configs/random_agent_config.yaml\")\n",
    "random_agent_config = yaml.safe_load(config_file)\n",
    "config_file.close()\n",
    "\n",
    "random_agent = RandomAgent(random_agent_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get RL agent ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config details from config file.\n",
    "config_file = open(\"configs/rl_agent_config.yaml\")\n",
    "rl_agent_config = yaml.safe_load(config_file)\n",
    "config_file.close()\n",
    "\n",
    "rl_agent = RLAgent(rl_agent_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test random agent on single games ##\n",
    "Test and evaluate the performance of the random agent on single games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# random_agent_testing_config = random_agent_config[\"testing\"]\n",
    "# random_agent_testing_stats = evaluate_performance(\n",
    "#     agent=random_agent, \n",
    "#     testing_game_config=game_config,\n",
    "#     training_game_config=None, \n",
    "#     training_enabled=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RL agent on single games ##\n",
    "Test and evaluate the performance of the RL agent on single games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs19s028/anaconda3/envs/research/lib/python3.8/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "\r",
      "  0%|          | 0/100 [00:00<?, ? epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4df64b5700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4df61e6430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|1         | 1/100 [03:32<5:51:05, 212.78s/ epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "=============================================\n",
      "Train stats\n",
      "--------------------------------------------\n",
      "Avg. reward: 0.267\n",
      "Avg. steps: 47.033\n",
      "Avg. wins: 0.267\n",
      "Epsilon: 0.978\n",
      "\n",
      "Test stats\n",
      "--------------------------------------------\n",
      "Avg. reward: 0.000\n",
      "Avg. steps: 50.000\n",
      "Avg. wins: 0.000\n",
      "Epsilon: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|1         | 1/100 [05:49<9:36:07, 349.17s/ epoch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-05c4c1ef87b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrl_agent_testing_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrl_agent_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"testing\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m rl_agent_testing_before_training_stats = evaluate_performance(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrl_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtesting_game_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgame_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtraining_game_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgame_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-52b3381661be>\u001b[0m in \u001b[0;36mevaluate_performance\u001b[0;34m(agent, training_game_config, testing_game_config, training_enabled)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtraining_game_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_game_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_game_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Stats containing info. related to the diff. epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ee03ff0d7c92>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, testing_game_info, training_game_info, training_enabled)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mepoch_training_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_game_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mepoch_testing_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_game_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ee03ff0d7c92>\u001b[0m in \u001b[0;36m_test\u001b[0;34m(self, testing_game_env)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0mv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_choose_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_testing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesting_game_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ee03ff0d7c92>\u001b[0m in \u001b[0;36m_choose_command\u001b[0;34m(self, state, eps)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# Exploit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_max_qs_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mv_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mn_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ee03ff0d7c92>\u001b[0m in \u001b[0;36m_get_max_qs_idx\u001b[0;34m(self, nn_model, state_batch)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mverb_scores_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoun_scores_batch\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mmax_verb_score_idx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverb_scores_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1570\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1725\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m     \"\"\"\n\u001b[0;32m-> 1727\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m   def interleave(self,\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4120\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4121\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4122\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   4123\u001b[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[1;32m   4124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m       \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \"\"\"\n\u001b[0;32m-> 2938\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   2939\u001b[0m         *args, **kwargs)\n\u001b[1;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3062\u001b[0m     ]\n\u001b[1;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3064\u001b[0;31m     graph_function = ConcreteFunction(\n\u001b[0m\u001b[1;32m   3065\u001b[0m         func_graph_module.func_graph_from_py_func(\n\u001b[1;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, shared_func_graph, function_spec)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;31m# These each get a reference to the FuncGraph deleter since they use the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m     \u001b[0;31m# FuncGraph directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1541\u001b[0;31m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[0m\u001b[1;32m   1542\u001b[0m         func_graph, self._attrs, self._garbage_collector)\n\u001b[1;32m   1543\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m     self._inference_function = _EagerDefinedFunction(\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[1;32m    461\u001b[0m       \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0mfunction_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m     \u001b[0mfunction_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rl_agent_testing_config = rl_agent_config[\"testing\"]\n",
    "rl_agent_testing_before_training_stats = evaluate_performance(\n",
    "    agent=rl_agent, \n",
    "    testing_game_config=game_config,\n",
    "    training_game_config=game_config,\n",
    "    training_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "playing_game_with_random_and_rl_agent_ACTIVE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
